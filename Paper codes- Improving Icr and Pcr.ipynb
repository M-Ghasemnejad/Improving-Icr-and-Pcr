{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d34df59",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example 1: \n",
    "#############################################################\n",
    "#eigenvalues and eigenvectors\n",
    "############################################\n",
    "\n",
    "import math    \n",
    "from scipy.stats import shapiro\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy.stats\n",
    "from sklearn.decomposition import FastICA\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from scipy.stats import pearsonr\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import skew\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.linalg import eig  \n",
    "    \n",
    "   \n",
    " # Generate a sample data (replace this with your actual data)\n",
    "#data = np.random.normal(0, 1, 100)\n",
    "    \n",
    "mean = np.array([0.766, 0.506, 0.438, 0.161]) \n",
    " #A = np.random.rand(5, 5)\n",
    "cov=np.array([[ 0.220588235, -148.253676, -2.53676471, -1.25000000,-1.13235294],\n",
    " [-148.253676,  3343690.26, 28424.4485, -37368.7500, 19935.2022],\n",
    " [-2.53676471,  28424.4485, 538.985294, -393.500000,195.272059],\n",
    " [-1.25000000, -37368.7500, -393.500000,  983.000000,-312.125000],\n",
    " [-1.13235294,  19935.2022,  195.272059, -312.125000 ,469.404412]])\n",
    "\n",
    "w,v=eig(cov)\n",
    "print('E-value:', w)\n",
    "print('E-vector', v)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4f026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "# Table 1:\n",
    " ## Ridge regression for PCs:\n",
    "    \n",
    "    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Data\n",
    "y = np.array([3389, 1101, 1131, 596, 896, 1767, 807, 1111, 645, 628, 1360, 652, 860, 500, 781, 1070, 1754])\n",
    "X = np.array([\n",
    "    [1, 7500, 220, 0, 140],\n",
    "    [1, 1975, 200, 0, 100],\n",
    "    [0, 3600, 205, 60, 111],\n",
    "    [1, 675, 160, 60, 120],\n",
    "    [1, 750, 185, 70, 83],\n",
    "    [1, 2500, 180, 60, 80],\n",
    "    [1, 350, 154, 80, 98],\n",
    "    [0, 1500, 200, 70, 93],\n",
    "    [1, 375, 137, 60, 105],\n",
    "    [1, 1050, 167, 60, 74],\n",
    "    [1, 3000, 180, 60, 80],\n",
    "    [1, 450, 160, 64, 60],\n",
    "    [1, 1750, 135, 90, 79],\n",
    "    [0, 2000, 160, 60, 80],\n",
    "    [0, 4500, 180, 0, 100],\n",
    "    [0, 1500, 170, 90, 120],\n",
    "    [1, 3000, 180, 0, 129]\n",
    "])\n",
    "\n",
    "# Fit Ridge Regression model\n",
    "ridge_model = Ridge(alpha=0.1)  # Alpha is the regularization parameter, can be adjusted\n",
    "ridge_model.fit(X, y)\n",
    "y_pred = ridge_model.predict(X)\n",
    "\n",
    "# Display R-squared, intercept, and coefficients\n",
    "print(\"R-squared (Ridge Regression):\", ridge_model.score(X, y))\n",
    "print(\"Intercept:\", ridge_model.intercept_)\n",
    "print(\"Coefficients:\", ridge_model.coef_)\n",
    "\n",
    "# Calculate error metrics\n",
    "mse = mean_squared_error(y, y_pred)  # Mean Squared Error\n",
    "mre = np.mean(np.abs(y - y_pred) / y) * 100  # Mean Relative Error in percentage\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Relative Error (%):\", mre)\n",
    "\n",
    "# Plot Actual vs Predicted values\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(y, y_pred, color='blue', alpha=0.6)\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', linewidth=2)  # 45-degree reference line\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs Predicted (Ridge Regression)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Residuals\n",
    "residuals = y - y_pred\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(y_pred, residuals, color='green', alpha=0.6)\n",
    "plt.axhline(y=0, color='r', linestyle='--', linewidth=2)  # Zero line for residuals\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot (Ridge Regression)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453edc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "## Lasso regression for PCs:\n",
    "###############################################\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Dependent variable\n",
    "y = np.array([3389, 1101, 1131, 596, 896, 1767, 807, 1111, 645, 628, 1360, 652, 860, 500, 781, 1070, 1754])\n",
    "\n",
    "# Original features (example: 5 features)\n",
    "X = np.array([\n",
    "    [1, 7500, 220, 0, 140],\n",
    "    [1, 1975, 200, 0, 100],\n",
    "    [0, 3600, 205, 60, 111],\n",
    "    [1, 675, 160, 60, 120],\n",
    "    [1, 750, 185, 70, 83],\n",
    "    [1, 2500, 180, 60, 80],\n",
    "    [1, 350, 154, 80, 98],\n",
    "    [0, 1500, 200, 70, 93],\n",
    "    [1, 375, 137, 60, 105],\n",
    "    [1, 1050, 167, 60, 74],\n",
    "    [1, 3000, 180, 60, 80],\n",
    "    [1, 450, 160, 64, 60],\n",
    "    [1, 1750, 135, 90, 79],\n",
    "    [0, 2000, 160, 60, 80],\n",
    "    [0, 4500, 180, 0, 100],\n",
    "    [0, 1500, 170, 90, 120],\n",
    "    [1, 3000, 180, 0, 129]\n",
    "])\n",
    "\n",
    "# Perform PCA and take the first principal component\n",
    "pca = PCA(n_components=1)\n",
    "pc = pca.fit_transform(X)  # Shape: (n_samples, 1)\n",
    "\n",
    "# Fit Lasso regression on the first principal component\n",
    "lasso_model = Lasso(alpha=0.1)  # Regularization parameter\n",
    "lasso_model.fit(pc, y)\n",
    "y_pred_lasso = lasso_model.predict(pc)\n",
    "\n",
    "# R-squared, intercept, and coefficient\n",
    "r_squared_lasso = lasso_model.score(pc, y)\n",
    "print(\"R-squared (Lasso Regression):\", r_squared_lasso)\n",
    "print(\"Intercept (Lasso Regression):\", lasso_model.intercept_)\n",
    "print(\"Coefficient (Lasso Regression):\", lasso_model.coef_)\n",
    "\n",
    "# Error metrics\n",
    "mse = mean_squared_error(y, y_pred_lasso)\n",
    "mre = np.mean(np.abs(y - y_pred_lasso) / y) * 100\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Relative Error (%):\", mre)\n",
    "\n",
    "# Plot Actual vs Predicted\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(y, y_pred_lasso, color='blue', alpha=0.6)\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', linewidth=2)  # 45-degree reference line\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs Predicted (Lasso Regression)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Residuals\n",
    "residuals = y - y_pred_lasso\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(y_pred_lasso, residuals, color='green', alpha=0.6)\n",
    "plt.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot (Lasso Regression)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b612bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Table1:\n",
    "##### Ridge Regression for ICs and y:\n",
    "############################################################\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import FastICA\n",
    "#from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Define the data matrix\n",
    "data = np.array([\n",
    "    [1, 7500, 220, 0, 140],\n",
    "    [1, 1975, 200, 0, 100],\n",
    "    [0, 3600, 205, 60, 111],\n",
    "    [1, 675, 160, 60, 120],\n",
    "    [1, 750, 185, 70, 83],\n",
    "    [1, 2500, 180, 60, 80],\n",
    "    [1, 350, 154, 80, 98],\n",
    "    [0, 1500, 200, 70, 93],\n",
    "    [1, 375, 137, 60, 105],\n",
    "    [1, 1050, 167, 60, 74],\n",
    "    [1, 3000, 180, 60, 80],\n",
    "    [1, 450, 160, 64, 60],\n",
    "    [1, 1750, 135, 90, 79],\n",
    "    [0, 2000, 160, 60, 80],\n",
    "    [0, 4500, 180, 0, 100],\n",
    "    [0, 1500, 170, 90, 120],\n",
    "    [1, 3000, 180, 0, 129]\n",
    "])\n",
    "\n",
    "\n",
    "# Center the data\n",
    "data_centered = data - np.mean(data, axis=0)\n",
    "\n",
    "# Calculate the covariance matrix\n",
    "cov_matrix = np.cov(data_centered, rowvar=False)\n",
    "\n",
    "# Perform eigenvalue decomposition of the covariance matrix\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "# Whitening transformation\n",
    "whitening_matrix = np.dot(eigenvectors, np.dot(np.diag(1.0/np.sqrt(eigenvalues + 1e-5)), eigenvectors.T))\n",
    "\n",
    "# Whiten the data\n",
    "w_d = np.dot(data_centered, whitening_matrix)\n",
    "Tw_d = np.transpose(w_d)\n",
    "print(w_d)\n",
    "correlation_coefficients = []\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "   # Generate some sample data\n",
    "   s1 = Tw_d[0]  # Signal 1 : sinusoidal signal\n",
    "   s2 = Tw_d[1]  # Signal 2 : square signal\n",
    "   s3 = Tw_d[2] \n",
    "   s4 = Tw_d[3]\n",
    "   s5 = Tw_d[4]\n",
    " \n",
    "  \n",
    "   ica = FastICA(n_components=5)\n",
    "   A = ica.fit_transform(Tw_d)\n",
    "\n",
    "   # Mix the signals into a single observed signal\n",
    "   #n=1030\n",
    "   S = np.c_[s1, s2, s3 , s4, s5]\n",
    "   X1 = np.dot(S, A.T)  # Observed signal\n",
    "   \n",
    "   import numpy as np\n",
    "   import matplotlib.pyplot as plt\n",
    "   # Perform ICA using the FastICA algorithm\n",
    "   ica = FastICA(n_components=5)\n",
    "   S_ = ica.fit_transform(X1)  # Reconstruct signals\n",
    "   \n",
    "\n",
    "   # Compare the reconstructed signals to the original signals\n",
    "   print(np.allclose(X1, np.dot(S_, ica.mixing_.T)))\n",
    "\n",
    "\n",
    "   TS_ = np.transpose(S_)\n",
    "   X = np.array([TS_[0], TS_[1] ,TS_[2], TS_[3], TS_[4]])\n",
    "   XT= np.transpose(X)\n",
    "   # Fit the model \n",
    "   #XT = sm.add_constant(XT) \n",
    "###########################################\n",
    "\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "y = np.array([3389, 1101, 1131, 596, 896, 1767, 807, 1111, 645, 628, 1360, 652, 860, 500, 781, 1070, 1754])\n",
    "# Fit the Ridge regression model\n",
    "ridge_regression_model = Ridge(alpha=0.1)  # Adjust the alpha parameter as needed\n",
    "ridge_regression_model.fit(XT, y)\n",
    "\n",
    "# Calculate predicted values\n",
    "y_pred_ridge = ridge_regression_model.predict(XT)\n",
    "\n",
    "# Calculate R-squared for Ridge regression\n",
    "r_squared_ridge = ridge_regression_model.score(XT, y)\n",
    "print(\"R-squared (Ridge Regression):\", r_squared_ridge)\n",
    "\n",
    "# Print intercept and coefficients for Ridge regression\n",
    "print(\"Intercept (Ridge Regression):\", ridge_regression_model.intercept_)\n",
    "print(\"Coefficients (Ridge Regression):\", ridge_regression_model.coef_)\n",
    "\n",
    "# Plot actual vs. predicted values\n",
    "plt.scatter(y, y_pred_ridge)\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs. Predicted Values (Ridge Regression)\")\n",
    "plt.show()\n",
    "\n",
    "# Plot residuals vs. predicted values\n",
    "residuals_ridge = y - y_pred_ridge\n",
    "plt.scatter(y_pred_ridge, residuals_ridge)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot (Ridge Regression)\")\n",
    "plt.axhline(y=0, color='r', linestyle='-')  # Add horizontal line at y=0\n",
    "plt.show()\n",
    "\n",
    "####################\n",
    "\n",
    " # Calculate mean squared error\n",
    "mse = mean_square_error(y, y_pred_ridge)\n",
    "print(\"Mean Square Error:\", mse)\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate mean relative error\n",
    "mre = mean_relative_error(y, y_pred_ridge)\n",
    "print(\"Mean Relative Error:\", mre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d092c198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table1:\n",
    "##### Lasso Regression in ICs,y components:\n",
    "################################################################\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.linear_model import Lasso\n",
    "import math    \n",
    "from scipy.stats import shapiro\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy.stats\n",
    "from sklearn.decomposition import FastICA\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from scipy.stats import pearsonr\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import skew\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Define the dependent variable y\n",
    "y = np.array([3389, 1101, 1131, 596, 896, 1767, 807, 1111, 645, 628, 1360, 652, 860, 500, 781, 1070, 1754])\n",
    "# Define the data matrix\n",
    "data = np.array([\n",
    "    [1, 7500, 220, 0, 140],\n",
    "    [1, 1975, 200, 0, 100],\n",
    "    [0, 3600, 205, 60, 111],\n",
    "    [1, 675, 160, 60, 120],\n",
    "    [1, 750, 185, 70, 83],\n",
    "    [1, 2500, 180, 60, 80],\n",
    "    [1, 350, 154, 80, 98],\n",
    "    [0, 1500, 200, 70, 93],\n",
    "    [1, 375, 137, 60, 105],\n",
    "    [1, 1050, 167, 60, 74],\n",
    "    [1, 3000, 180, 60, 80],\n",
    "    [1, 450, 160, 64, 60],\n",
    "    [1, 1750, 135, 90, 79],\n",
    "    [0, 2000, 160, 60, 80],\n",
    "    [0, 4500, 180, 0, 100],\n",
    "    [0, 1500, 170, 90, 120],\n",
    "    [1, 3000, 180, 0, 129]\n",
    "])\n",
    "\n",
    "\n",
    "# Center the data\n",
    "data_centered = data - np.mean(data, axis=0)\n",
    "\n",
    "# Calculate the covariance matrix\n",
    "cov_matrix = np.cov(data_centered, rowvar=False)\n",
    "\n",
    "# Perform eigenvalue decomposition of the covariance matrix\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "# Whitening transformation\n",
    "whitening_matrix = np.dot(eigenvectors, np.dot(np.diag(1.0/np.sqrt(eigenvalues + 1e-5)), eigenvectors.T))\n",
    "\n",
    "# Whiten the data\n",
    "w_d = np.dot(data_centered, whitening_matrix)\n",
    "Tw_d = np.transpose(w_d)\n",
    "print(w_d)\n",
    "correlation_coefficients = []\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "   # Generate some sample data\n",
    "   s1 = Tw_d[0]  # Signal 1 : sinusoidal signal\n",
    "   s2 = Tw_d[1]  # Signal 2 : square signal\n",
    "   s3 = Tw_d[2] \n",
    "   s4 = Tw_d[3]\n",
    "   s5 = Tw_d[4]\n",
    " \n",
    "  \n",
    "   ica = FastICA(n_components=5)\n",
    "   A = ica.fit_transform(Tw_d)\n",
    "\n",
    "   # Mix the signals into a single observed signal\n",
    "   #n=1030\n",
    "   S = np.c_[s1, s2, s3 , s4, s5]\n",
    "   X1 = np.dot(S, A.T)  # Observed signal\n",
    "   \n",
    "   import numpy as np\n",
    "   import matplotlib.pyplot as plt\n",
    "   # Perform ICA using the FastICA algorithm\n",
    "   ica = FastICA(n_components=5)\n",
    "   S_ = ica.fit_transform(X1)  # Reconstruct signals\n",
    "   \n",
    "\n",
    "   # Compare the reconstructed signals to the original signals\n",
    "   print(np.allclose(X1, np.dot(S_, ica.mixing_.T)))\n",
    "\n",
    "\n",
    "   TS_ = np.transpose(S_)\n",
    "   X = np.array([TS_[0], TS_[1] ,TS_[2], TS_[3], TS_[4]])\n",
    "   XT= np.transpose(X)\n",
    "#############################    \n",
    "  \n",
    "# mre  #####################\n",
    "#calculate MRE:\n",
    "\n",
    "def mean_relative_error(actual1, predicted1):\n",
    "    \n",
    "   actual1, predicted1 = np.array(actual1), np.array(predicted1)\n",
    "   return np.mean(np.abs((actual1 - predicted1) / actual1))\n",
    "\n",
    "#########################\n",
    " # Calculate mean squared error\n",
    "def mean_square_error(actual_values, predicted_values):\n",
    "   \n",
    "    # Check if actual_values and predicted_values have the same length\n",
    "   if len(actual_values) != len(predicted_values):\n",
    "    raise ValueError(\"Length of actual_values and predicted_values must be the same.\")\n",
    "    \n",
    "    # Calculate MSE\n",
    "   mse = sum((actual - predicted) ** 2 for actual, predicted in zip(actual_values, predicted_values)) / len(actual_values)\n",
    "    \n",
    "   return mse\n",
    "\n",
    "\n",
    "############################\n",
    "    \n",
    "###############################    \n",
    "# Fit the Lasso regression model\n",
    "lasso_regression_model = Lasso(alpha=0.1)  # Adjust the alpha parameter as needed\n",
    "lasso_regression_model.fit(XT, y)\n",
    "\n",
    "# Calculate predicted values\n",
    "y_pred_lasso = lasso_regression_model.predict(XT)\n",
    "\n",
    "# Calculate R-squared for Lasso regression\n",
    "r_squared_lasso = lasso_regression_model.score(XT, y)\n",
    "print(\"R-squared (Lasso Regression):\", r_squared_lasso)\n",
    "\n",
    "# Print intercept and coefficients for Lasso regression\n",
    "print(\"Intercept (Lasso Regression):\", lasso_regression_model.intercept_)\n",
    "print(\"Coefficients (Lasso Regression):\", lasso_regression_model.coef_)\n",
    "\n",
    "# Plot actual vs. predicted values\n",
    "plt.scatter(y, y_pred_lasso)\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs. Predicted Values (Lasso Regression)\")\n",
    "plt.show()\n",
    "\n",
    "# Plot residuals vs. predicted values\n",
    "residuals_lasso = y - y_pred_lasso\n",
    "plt.scatter(y_pred_lasso, residuals_lasso)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot (Lasso Regression)\")\n",
    "plt.axhline(y=0, color='r', linestyle='-')  # Add horizontal line at y=0\n",
    "plt.show()\n",
    "\n",
    " # Calculate mean squared error\n",
    "mse = mean_square_error(y, y_pred_lasso)\n",
    "print(\"Mean Square Error:\", mse)\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate mean relative error\n",
    "mre = mean_relative_error(y, y_pred_lasso)\n",
    "print(\"Mean Relative Error:\", mre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c37f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simulation##  Table 2 (PCR)\n",
    "\n",
    "#(True)  Pcr for 4 component (random coefficient):# for witening data: for simulation data\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "import math    \n",
    "from scipy.stats import shapiro\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy.stats\n",
    "from sklearn.decomposition import FastICA\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from scipy.stats import pearsonr\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import skew\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "   \n",
    "    \n",
    "   \n",
    " # Generate a sample data (replace this with your actual data)\n",
    "#data = np.random.normal(0, 1, 100)\n",
    "    \n",
    "mean = np.array([0.04568769, 0.03916346, 0.10603323, 0.60737812, 0.21861233]) \n",
    " #A = np.random.rand(5, 5)\n",
    "A=np.array([[0.54407526, 0.01804927 ,0.95803021 ,0.53246229, 0.98903702],\n",
    "     [0.85178268, 0.27275685 ,0.8007421  ,0.53891002, 0.07267785],\n",
    "     [0.0977354 , 0.86782946 ,0.09019516 ,0.59581037, 0.97433572],\n",
    "     [0.69733138, 0.27297179 ,0.69968404 ,0.33733182, 0.96135071],\n",
    "     [0.98289599, 0.01509631 ,0.150057   ,0.346731  , 0.85385711]])\n",
    "    \n",
    "cov = np.dot(A, A.T)\n",
    "    # Generate random samples from the multivariate normal distribution\n",
    "num_samples = 5000\n",
    "\n",
    "def root_mean_squared_error(actual, predicted):\n",
    "    if len(actual) != len(predicted):\n",
    "        raise ValueError(\"Length of actual and predicted lists must be the same.\")\n",
    "    squared_errors = [(actual[k] - predicted[k]) ** 2 for k in range(len(actual))]\n",
    "    mean_squared_error = sum(squared_errors) / len(actual)\n",
    "    rmse = math.sqrt(mean_squared_error)\n",
    "    return rmse\n",
    "\n",
    "\n",
    "def mean_relative_error(actual1, predicted1):\n",
    "    \n",
    "    actual1, predicted1 = np.array(actual1), np.array(predicted1)\n",
    "    return np.mean(np.abs((actual1 - predicted1) / actual1))\n",
    "\n",
    "rmse_sum = 0\n",
    "mre_sum = 0\n",
    "mse_sum = 0\n",
    "\n",
    "\n",
    "mse_sum = 0\n",
    "for j in range(5000):\n",
    "      samples = np.random.multivariate_normal(mean, cov, num_samples)\n",
    "    \n",
    "    # Define the coefficients for the linear combination\n",
    "      coefficients = np.random.rand(5)\n",
    "    #coefficients =np.array([0.89931776, 0.35102325, 0.55151473, 0.21779235, 0.89916957])\n",
    "    # Generate the response variable as a linear combination of variables with noise\n",
    "      noise = np.random.normal(0, 1, num_samples)  # Additive noise term\n",
    "      response = np.dot(samples, coefficients) + noise\n",
    "   \n",
    "    # Display the first few response values\n",
    "    #print(response[:5])\n",
    "    #print(coefficients)\n",
    "    #plt.hist(samples[:500])\n",
    "    #plt.hist(response[:500])\n",
    "    # Perform the Shapiro-Wilk test\n",
    "      statistic, p_value = shapiro(response)\n",
    "    # Display the test statistic and p-value\n",
    "      print(\"Test statistic:\", statistic)\n",
    "      print(\"P-value:\", p_value)\n",
    "   \n",
    "      if p_value > 0.05:\n",
    "        print(\"The data is normally distributed (fail to reject H0)\")\n",
    "      else:\n",
    "        print(\"The data is not normally distributed (reject H0)\")\n",
    "      \n",
    "   \n",
    "    # Split the real data into independent and dependent variables\n",
    "      X_real =samples\n",
    "      y_real = response\n",
    "   \n",
    "    # Perform principal component analysis\n",
    "      pca = PCA(n_components=5)  # Replace 8 with the desired number of principal components\n",
    "      X_pca = pca.fit_transform(X_real)\n",
    "   \n",
    "    # Calculate the correlation coefficients of the principal components with the original variables\n",
    "      correlation_coefficients = np.corrcoef(X_pca.T, X_real.T)\n",
    "      print(\"Correlation Coefficients of Principal Components with Original Variables:\")\n",
    "      print(correlation_coefficients)\n",
    "   \n",
    "      pc = np.zeros((len(samples), pca.n_components_))\n",
    "    # Individual principal components\n",
    "      print(\"Individual Principal Components:\")\n",
    "      for i in range(pca.n_components_):\n",
    "        print(f\"Principal Component {i+1}: {pca.components_[i]}\")\n",
    "        pc[:, i] = np.dot(samples, pca.components_[i])\n",
    "      print(\"pc:\")\n",
    "      print(pc)\n",
    "   \n",
    "    # Eigenvalues of the principal components\n",
    "      print(\"Eigenvalues of Principal Components:\")\n",
    "      print(pca.explained_variance_)\n",
    "   \n",
    "    # Matrix of correlation coefficients of the principal components with the original variables\n",
    "      print(\"Matrix of Correlation Coefficients of Principal Components with Original Variables:\")\n",
    "      print(correlation_coefficients)\n",
    "    #print(pca.components_[1])\n",
    "    #############################################################\n",
    "    #pcr for test , train data:\n",
    "    #X_real_reshaped = X_real.reshape(-1,1)\n",
    "      X_real = np.array([pc[:,0], pc[:,1], pc[:,2], pc[:,3] ])\n",
    "      TX_real= np.transpose(X_real)\n",
    "      y_real = response\n",
    "      mse_list = []\n",
    "    #for i in range(2):\n",
    "    # Perform principal component analysis\\n\",\n",
    "      pca = PCA(n_components=4)  # Replace 2 with the desired number of principal components\n",
    "    #X_pca = pca.fit_transform(X_real)\n",
    "      X_pca = pca.fit_transform(TX_real)\n",
    "    \n",
    "      X_pca_train, X_pca_test, y_pca_train, y_pca_test = train_test_split(X_pca, y_real, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Perform linear regression on the PCA data\n",
    "      X_pca_train = sm.add_constant(X_pca_train)  # Add a constant term to the independent variables\n",
    "      reg_pca = sm.OLS(y_pca_train, X_pca_train).fit()  # Fit the linear regression model\n",
    "      X_pca_test = sm.add_constant(X_pca_test)\n",
    "      y_pred = reg_pca.predict(X_pca_test)\n",
    "\n",
    "     # Calculate mean squared error\n",
    "      mse = mean_squared_error(y_pca_test, y_pred)\n",
    "      print(\"Mean Squared Error:\", mse)\n",
    "      mse_sum += mse\n",
    "\n",
    "    # Calculate root mean squared error\n",
    "      rmse = root_mean_squared_error(y_pca_test, y_pred)\n",
    "      print(\"Root Mean Squared Error:\", rmse)\n",
    "      rmse_sum += rmse\n",
    "\n",
    "    # Calculate mean relative error\n",
    "      mre = mean_relative_error(y_pca_test, y_pred)\n",
    "      print(\"Mean Relative Error:\", mre)\n",
    "      mre_sum += mre\n",
    "    \n",
    "avg_mse = mse_sum / 5000\n",
    "avg_rmse = rmse_sum / 5000\n",
    "avg_mre = mre_sum / 5000\n",
    "\n",
    "print(\"Average Mean Squared Error:\", avg_mse)\n",
    "print(\"Average Root Mean Squared Error:\", avg_rmse)\n",
    "print(\"Average Mean relative Error:\", avg_mre)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775e2158",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simulation## Table 2 (ICA):\n",
    "\n",
    "##  Icr for 4 component (random coefficient): for simulation data \n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "import math   \n",
    "from scipy.stats import shapiro\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "mean = np.array([0.04568769, 0.03916346, 0.10603323, 0.60737812, 0.21861233]) \n",
    "A = np.array([[0.54407526, 0.01804927, 0.95803021, 0.53246229, 0.98903702],\n",
    "              [0.85178268, 0.27275685, 0.8007421,  0.53891002, 0.07267785],\n",
    "              [0.0977354,  0.86782946, 0.09019516, 0.59581037, 0.97433572],\n",
    "              [0.69733138, 0.27297179, 0.69968404, 0.33733182, 0.96135071],\n",
    "              [0.98289599, 0.01509631, 0.150057,   0.346731,   0.85385711]])\n",
    "\n",
    "cov = np.dot(A, A.T)\n",
    "num_samples = 5000\n",
    "\n",
    "def root_mean_squared_error(actual, predicted):\n",
    "    if len(actual) != len(predicted):\n",
    "        raise ValueError(\"Length of actual and predicted lists must be the same.\")\n",
    "    squared_errors = [(actual[k] - predicted[k]) ** 2 for k in range(len(actual))]\n",
    "    mean_squared_error = sum(squared_errors) / len(actual)\n",
    "    rmse = math.sqrt(mean_squared_error)\n",
    "    return rmse\n",
    "\n",
    "\n",
    "def mean_relative_error(actual1, predicted1):\n",
    "    \n",
    "    actual1, predicted1 = np.array(actual1), np.array(predicted1)\n",
    "    return np.mean(np.abs((actual1 - predicted1) / actual1))\n",
    "\n",
    "rmse_sum = 0\n",
    "mre_sum = 0\n",
    "mse_sum = 0\n",
    "for j in range(5000):\n",
    "    samples = np.random.multivariate_normal(mean, cov, num_samples)\n",
    "    coefficients = np.random.rand(5)\n",
    "    noise = np.random.normal(0, 1, num_samples)  # Additive noise term\n",
    "    response = np.dot(samples, coefficients) + noise\n",
    "\n",
    "    X_real = samples\n",
    "    y_real = response\n",
    "    \n",
    "    X_real_centered = X_real - np.mean(X_real, axis=0)\n",
    "    cov_matrix = np.cov(X_real_centered, rowvar=False)\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "    whitening_matrix = np.dot(eigenvectors, np.dot(np.diag(1.0 / np.sqrt(eigenvalues + 1e-5)), eigenvectors.T))\n",
    "    w_d = np.dot(X_real_centered, whitening_matrix)\n",
    "    Tw_d = np.transpose(w_d)\n",
    "    \n",
    "    s1 = Tw_d[0]\n",
    "    s2 = Tw_d[1]\n",
    "    s3 = Tw_d[2]\n",
    "    s4 = Tw_d[3]\n",
    "    s5 = Tw_d[4]\n",
    "    \n",
    "    ica = FastICA(n_components=5)\n",
    "    A = ica.fit_transform(Tw_d)\n",
    "    \n",
    "    S = np.c_[s1, s2, s3, s4, s5]\n",
    "    X1 = np.dot(S, A.T)\n",
    "    S_ = ica.fit_transform(X1)\n",
    "    TS_ = np.transpose(w_d)\n",
    "    \n",
    "    Y = y_real\n",
    "    X = np.array([TS_[0], TS_[1], TS_[2], TS_[3], TS_[4]])\n",
    "    XT = np.transpose(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(XT, Y, test_size=0.2, random_state=42)\n",
    "    X_train = sm.add_constant(X_train)\n",
    "    model = sm.OLS(y_train, X_train).fit()\n",
    "    X_test = sm.add_constant(X_test)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    r_squared = r2_score(y_test, y_pred)\n",
    "    print(\"R-squared:\", r_squared)\n",
    "    print(model.summary())\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(\"Mean Squared Error:\", mse)\n",
    "    mse_sum += mse\n",
    "    \n",
    "    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "    print(\"Root Mean Squared Error:\", rmse)\n",
    "    rmse_sum += rmse\n",
    "    \n",
    "    mre = mean_relative_error(y_test, y_pred)\n",
    "    print(\"Mean Relative Error:\", mre)\n",
    "    mre_sum += mre\n",
    "    \n",
    "avg_mse = mse_sum / 5000\n",
    "avg_rmse = rmse_sum / 5000\n",
    "avg_mre = mre_sum / 5000\n",
    "\n",
    "print(\"Average Mean Squared Error:\", avg_mse)\n",
    "print(\"Average Root Mean Squared Error:\", avg_rmse)\n",
    "print(\"Average Mean relative Error:\", avg_mre)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12af5d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simulation## Table 3:\n",
    "\n",
    "## True AIC, BIC, MSE, RMSE, MRE FOR ICR,PcR\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Error metrics functions\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def mean_relative_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true))\n",
    "\n",
    "# AIC and BIC Calculation functions\n",
    "def calculate_aic(n, rss, k):\n",
    "    return n * np.log(rss / n) + 2 * k\n",
    "\n",
    "def calculate_bic(n, rss, k):\n",
    "    return n * np.log(rss / n) + k * np.log(n)\n",
    "\n",
    "# Function to apply Kaiser Criterion\n",
    "def kaiser_criterion(eigenvalues):\n",
    "    return sum(eigenvalues > 1)\n",
    "\n",
    "# Function to apply Explained Variance Criterion\n",
    "def explained_variance_criterion(pca, threshold=0.90):\n",
    "    explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    return np.argmax(explained_variance >= threshold) + 1\n",
    "\n",
    "# Function to perform Horn’s Parallel Analysis\n",
    "def horns_parallel_analysis(X, num_simulations=5000):\n",
    "    num_samples, num_features = X.shape\n",
    "    eigenvalues = np.linalg.eigvals(np.cov(X.T))\n",
    "    eigenvalues_sorted = np.sort(eigenvalues)[::-1]\n",
    "    \n",
    "    # Generate random data and compute eigenvalues for multiple simulations\n",
    "    random_eigenvalues = np.zeros((num_simulations, num_features))\n",
    "    for i in range(num_simulations):\n",
    "        random_data = np.random.normal(size=(num_samples, num_features))\n",
    "        random_eigenvalues[i] = np.sort(np.linalg.eigvals(np.cov(random_data.T)))[::-1]\n",
    "    \n",
    "    # Compute the critical eigenvalue threshold\n",
    "    threshold = np.percentile(random_eigenvalues, 95, axis=0)\n",
    "    \n",
    "    # Count the number of eigenvalues greater than the threshold\n",
    "    return sum(eigenvalues_sorted > threshold[0])\n",
    "\n",
    "# Initialize metrics for ICA and PCA models\n",
    "metrics_sums = {\n",
    "    \"ICA-Lasso\": {\"MSE\": 0, \"RMSE\": 0, \"MRE\": 0, \"AIC\": 0, \"BIC\": 0},\n",
    "    \"ICA-Ridge\": {\"MSE\": 0, \"RMSE\": 0, \"MRE\": 0, \"AIC\": 0, \"BIC\": 0},\n",
    "    \"ICA-OLS\": {\"MSE\": 0, \"RMSE\": 0, \"MRE\": 0, \"AIC\": 0, \"BIC\": 0},\n",
    "    \"PCA-Lasso\": {\"MSE\": 0, \"RMSE\": 0, \"MRE\": 0, \"AIC\": 0, \"BIC\": 0},\n",
    "    \"PCA-Ridge\": {\"MSE\": 0, \"RMSE\": 0, \"MRE\": 0, \"AIC\": 0, \"BIC\": 0},\n",
    "    \"PCA-OLS\": {\"MSE\": 0, \"RMSE\": 0, \"MRE\": 0, \"AIC\": 0, \"BIC\": 0},\n",
    "}\n",
    "\n",
    "# Simulation parameters\n",
    "num_simulations = 5000\n",
    "num_samples = 5000\n",
    "mean = np.zeros(5)\n",
    "cov = np.eye(5)\n",
    "\n",
    "for _ in range(num_simulations):\n",
    "    # Generate synthetic data\n",
    "    samples = np.random.multivariate_normal(mean, cov, num_samples)\n",
    "    coefficients = np.random.rand(5)\n",
    "    noise = np.random.normal(0, 1, num_samples)\n",
    "    response = np.dot(samples, coefficients) + noise\n",
    "\n",
    "    # Outlier detection and removal using IsolationForest\n",
    "    isolation_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "    outlier_labels = isolation_forest.fit_predict(samples)\n",
    "    clean_indices = outlier_labels == 1\n",
    "    samples_cleaned = samples[clean_indices]\n",
    "    response_cleaned = response[clean_indices]\n",
    "\n",
    "    # Standardize data\n",
    "    scaler = StandardScaler()\n",
    "    samples_standardized = scaler.fit_transform(samples_cleaned)\n",
    "\n",
    "    # Determine the number of components for PCA and ICA based on different criteria\n",
    "    pca = PCA()\n",
    "    pca.fit(samples_standardized)\n",
    "    \n",
    "    # Apply Kaiser Criterion to PCA\n",
    "    num_components_pca_kaiser = kaiser_criterion(pca.explained_variance_)\n",
    "\n",
    "    # Apply Explained Variance Criterion to PCA\n",
    "    num_components_pca_variance = explained_variance_criterion(pca, threshold=0.90)\n",
    "    \n",
    "    # Apply Horn's Parallel Analysis to PCA\n",
    "    num_components_pca_horn = horns_parallel_analysis(samples_standardized)\n",
    "    \n",
    "    # Perform ICA with determined number of components\n",
    "    ica = FastICA(n_components=num_components_pca_variance, random_state=42, max_iter=1000, tol=0.001)\n",
    "    X_ica = ica.fit_transform(samples_standardized)\n",
    "\n",
    "    # Perform PCA with determined number of components\n",
    "    pca = PCA(n_components=num_components_pca_kaiser)\n",
    "    X_pca = pca.fit_transform(samples_standardized)\n",
    "\n",
    "    # Split data for ICA and PCA\n",
    "    X_ica_train, X_ica_test, y_train, y_test = train_test_split(\n",
    "        X_ica, response_cleaned, test_size=0.2, random_state=42\n",
    "    )\n",
    "    X_pca_train, X_pca_test, _, _ = train_test_split(\n",
    "        X_pca, response_cleaned, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Regression models for ICA and PCA\n",
    "    for method_prefix, X_train, X_test in zip(\n",
    "        [\"ICA\", \"PCA\"],\n",
    "        [X_ica_train, X_pca_train],\n",
    "        [X_ica_test, X_pca_test],\n",
    "    ):\n",
    "        # Lasso Regression\n",
    "        lasso = Lasso(alpha=0.1, random_state=42)\n",
    "        lasso.fit(X_train, y_train)\n",
    "        y_pred_lasso = lasso.predict(X_test)\n",
    "        rss_lasso = np.sum((y_test - y_pred_lasso) ** 2)\n",
    "        aic_lasso = calculate_aic(len(y_test), rss_lasso, lasso.coef_.shape[0] + 1)\n",
    "        bic_lasso = calculate_bic(len(y_test), rss_lasso, lasso.coef_.shape[0] + 1)\n",
    "\n",
    "        # Ridge Regression\n",
    "        ridge = Ridge(alpha=1.0, random_state=42)\n",
    "        ridge.fit(X_train, y_train)\n",
    "        y_pred_ridge = ridge.predict(X_test)\n",
    "        rss_ridge = np.sum((y_test - y_pred_ridge) ** 2)\n",
    "        aic_ridge = calculate_aic(len(y_test), rss_ridge, ridge.coef_.shape[0] + 1)\n",
    "        bic_ridge = calculate_bic(len(y_test), rss_ridge, ridge.coef_.shape[0] + 1)\n",
    "\n",
    "        # OLS Regression\n",
    "        ols = LinearRegression()\n",
    "        ols.fit(X_train, y_train)\n",
    "        y_pred_ols = ols.predict(X_test)\n",
    "        rss_ols = np.sum((y_test - y_pred_ols) ** 2)\n",
    "        aic_ols = calculate_aic(len(y_test), rss_ols, ols.coef_.shape[0] + 1)\n",
    "        bic_ols = calculate_bic(len(y_test), rss_ols, ols.coef_.shape[0] + 1)\n",
    "\n",
    "        # Calculate and accumulate metrics\n",
    "        for method_suffix, y_pred, aic, bic in zip(\n",
    "            [\"Lasso\", \"Ridge\", \"OLS\"], [y_pred_lasso, y_pred_ridge, y_pred_ols],\n",
    "            [aic_lasso, aic_ridge, aic_ols], [bic_lasso, bic_ridge, bic_ols]\n",
    "        ):\n",
    "            method = f\"{method_prefix}-{method_suffix}\"\n",
    "            metrics_sums[method][\"MSE\"] += mean_squared_error(y_test, y_pred)\n",
    "            metrics_sums[method][\"RMSE\"] += root_mean_squared_error(y_test, y_pred)\n",
    "            metrics_sums[method][\"MRE\"] += mean_relative_error(y_test, y_pred)\n",
    "            metrics_sums[method][\"AIC\"] += aic\n",
    "            metrics_sums[method][\"BIC\"] += bic\n",
    "\n",
    "# Calculate averages\n",
    "metrics_avgs = {\n",
    "    method: {metric: total / num_simulations for metric, total in metrics.items()}\n",
    "    for method, metrics in metrics_sums.items()\n",
    "}\n",
    "\n",
    "# Print results\n",
    "for method, metrics in metrics_avgs.items():\n",
    "    print(f\"{method}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Visualization of results\n",
    "methods = list(metrics_avgs.keys())\n",
    "mse_values = [metrics[\"MSE\"] for metrics in metrics_avgs.values()]\n",
    "rmse_values = [metrics[\"RMSE\"] for metrics in metrics_avgs.values()]\n",
    "mre_values = [metrics[\"MRE\"] for metrics in metrics_avgs.values()]\n",
    "aic_values = [metrics[\"AIC\"] for metrics in metrics_avgs.values()]\n",
    "bic_values = [metrics[\"BIC\"] for metrics in metrics_avgs.values()]\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.subplot(1, 5, 1)\n",
    "plt.bar(methods, mse_values, color=['blue', 'orange', 'green', 'red', 'purple', 'brown'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Average MSE Comparison')\n",
    "plt.ylabel('MSE')\n",
    "\n",
    "plt.subplot(1, 5, 2)\n",
    "plt.bar(methods, rmse_values, color=['blue', 'orange', 'green', 'red', 'purple', 'brown'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Average RMSE Comparison')\n",
    "plt.ylabel('RMSE')\n",
    "\n",
    "plt.subplot(1, 5, 3)\n",
    "plt.bar(methods, mre_values, color=['blue', 'orange', 'green', 'red', 'purple', 'brown'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Average MRE Comparison')\n",
    "plt.ylabel('MRE')\n",
    "\n",
    "plt.subplot(1, 5, 4)\n",
    "plt.bar(methods, aic_values, color=['blue', 'orange', 'green', 'red', 'purple', 'brown'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Average AIC Comparison')\n",
    "plt.ylabel('AIC')\n",
    "\n",
    "plt.subplot(1, 5, 5)\n",
    "plt.bar(methods, bic_values, color=['blue', 'orange', 'green', 'red', 'purple', 'brown'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Average BIC Comparison')\n",
    "plt.ylabel('BIC')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f30842",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Figure 1\n",
    "\n",
    "############################################################################\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "import numpy as np\n",
    "from sklearn.decomposition import FastICA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import statsmodels.api as sm\n",
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from scipy.stats import pearsonr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy.stats import skew\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "class CustomFastICA:\n",
    "    def __init__(self, n_components, df, loc, scale, max_iter=200, tol=1e-4):\n",
    "        self.n_components = n_components\n",
    "        self.df = df  # degrees of freedom for the t-distribution\n",
    "        self.loc = loc  # mean\n",
    "        self.scale = scale  # standard deviation\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "\n",
    "    def fit(self, X):\n",
    "        n, m = X.shape\n",
    "        W = np.random.rand(self.n_components, m)\n",
    "\n",
    "        for iteration in range(self.max_iter):\n",
    "            W_old = W.copy()\n",
    "\n",
    "            # Calculate new weights\n",
    "            Y = np.dot(X, W.T)\n",
    "            g = self._tanh(Y)\n",
    "            g_dot = self._tanh_derivative(Y)\n",
    "            W = (1 / n) * np.dot(g.T, X) - np.mean(g_dot) * W\n",
    "\n",
    "            # Symmetric orthogonalization\n",
    "            W = self._symmetric_orthogonalization(W)\n",
    "\n",
    "            # Decorrelation\n",
    "            W = self._decorrelation(W)\n",
    "\n",
    "            # Check convergence\n",
    "            if iteration > 0 and np.linalg.norm(W - W_old) < self.tol:\n",
    "                break\n",
    "\n",
    "        self.components_ = W\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.dot(X, self.components_.T)\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def _tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def _tanh_derivative(self, x):\n",
    "        return 1 - np.tanh(x) ** 2\n",
    "\n",
    "    def _symmetric_orthogonalization(self, W):\n",
    "        u, s, vh = np.linalg.svd(W, full_matrices=False)\n",
    "        return np.dot(u, vh)\n",
    "\n",
    "    def _decorrelation(self, W):\n",
    "        WTW = np.dot(W, W.T)\n",
    "        _, WTW_eigenvecs = np.linalg.eigh(WTW)\n",
    "        return np.dot(WTW_eigenvecs.T, W)\n",
    "    ###########################################################################\n",
    "\n",
    "# Load dataset\n",
    "df=pd.read_csv('data1.csv')\n",
    "X = df.values\n",
    "# Center the data\n",
    "X_centered = X - np.mean(X, axis=0)\n",
    "\n",
    "# Calculate the covariance matrix\n",
    "cov_matrix = np.cov(X_centered, rowvar=False)\n",
    "\n",
    "# Perform eigenvalue decomposition of the covariance matrix\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "# Whitening transformation\n",
    "whitening_matrix = np.dot(eigenvectors, np.dot(np.diag(1.0/np.sqrt(eigenvalues + 1e-5)), eigenvectors.T))\n",
    "\n",
    "# Whiten the data\n",
    "w_d = np.dot(X_centered, whitening_matrix)\n",
    "Tw_d = np.transpose(w_d)\n",
    "\n",
    "#############################################################\n",
    "# Defining signals (input variables)\n",
    "s11 = df['Cement ']  \n",
    "s22 = df['Blast Furnace Slag']  \n",
    "s33 = df['Fly Ash ']  \n",
    "s44 = df['Water  ']\n",
    "s55 = df['Superplasticize']\n",
    "s66 = df['Coarse Aggregate  \\n\\n']\n",
    "s77 = df['Fine Aggregate']\n",
    "s88 = df['Age ']\n",
    "s99 = df['Concrete compressive strength']\n",
    "\n",
    "ica = FastICA(n_components=8)\n",
    "A = ica.fit_transform(df)\n",
    "S = np.c_[s11, s22, s33 , s44 , s55, s66, s77, s88 ]\n",
    "X1 = np.dot(S, A.T) \n",
    "X1T= np.transpose(X1)\n",
    "\n",
    "##########################################################\n",
    "correlation_coefficients = []\n",
    "\n",
    "s1 = Tw_d[0]  \n",
    "s2 = Tw_d[1]  \n",
    "s3 = Tw_d[2] \n",
    "s4 = Tw_d[3]\n",
    "s5 = Tw_d[4]\n",
    "s6 = Tw_d[5] \n",
    "s7 = Tw_d[6] \n",
    "s8 = Tw_d[7] \n",
    "s9 = Tw_d[8] \n",
    "\n",
    "S = np.c_[s1, s2, s3 , s4 , s5, s6, s7, s8]\n",
    "\n",
    "##################\n",
    "# Instantiate and fit the CustomFastICA model\n",
    "custom_ica = CustomFastICA(n_components=8, df=8.869463345651127, loc=-0.0364849978662904, scale=0.8750974313065253)\n",
    "S_ = custom_ica.fit_transform(S)\n",
    "\n",
    "y = df['Concrete compressive strength']\n",
    "X = np.array([S_[:, 0],S_[:, 1],S_[:, 2], S_[:, 3],S_[:, 4],S_[:, 5],S_[:, 6],S_[:, 7]])\n",
    "XT= np.transpose(X)\n",
    "XT = sm.add_constant(XT)  \n",
    "model = sm.OLS(y, XT).fit()\n",
    "########################################################\n",
    "\n",
    "# ... (middle parts of the code remain unchanged)\n",
    "\n",
    "#########################################3\n",
    "\n",
    "###############################################\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Independent components (ICs): S_\n",
    "df_IC = pd.DataFrame(S_, columns=[f\"IC{i+1}\" for i in range(S_.shape[1])])\n",
    "\n",
    "# Scatter function with nonlinear polynomial fitting\n",
    "def scatter_with_polyfit(x, y, degree=2, **kwargs):\n",
    "    plt.scatter(x, y, alpha=0.5, color='gray')\n",
    "    # Polynomial fitting\n",
    "    x_poly = x.values.reshape(-1, 1) if hasattr(x, 'values') else x.reshape(-1, 1)\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X_poly = poly.fit_transform(x_poly)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y)\n",
    "    # Plot fitted curve\n",
    "    x_fit = np.linspace(x_poly.min(), x_poly.max(), 200).reshape(-1, 1)\n",
    "    y_fit = model.predict(poly.transform(x_fit))\n",
    "    plt.plot(x_fit, y_fit, color='red', linewidth=3)\n",
    "\n",
    "# Pairplot\n",
    "pairplot_fig = sns.PairGrid(df_IC)\n",
    "pairplot_fig.map_offdiag(scatter_with_polyfit)\n",
    "pairplot_fig.map_diag(plt.hist, color='gray', edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Label ICs on axes\n",
    "for ax, col in zip(pairplot_fig.axes[-1], df_IC.columns):\n",
    "    ax.set_xlabel(col, fontsize=12, fontweight='bold')\n",
    "for ax, row in zip(pairplot_fig.axes[:,0], df_IC.columns):\n",
    "    ax.set_ylabel(row, fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "##################################################\n",
    "\n",
    "##################################################\n",
    "# Scatter plots of each IC vs dependent variable with nonlinear fitting\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))  \n",
    "axes = axes.flatten()  \n",
    "\n",
    "for i in range(S_.shape[1]):\n",
    "    x = S_[:, i]\n",
    "    axes[i].scatter(x, y, alpha=0.5)\n",
    "    \n",
    "    # Nonlinear polynomial fitting (degree=2)\n",
    "    x_poly = x.reshape(-1, 1)\n",
    "    poly = PolynomialFeatures(degree=2)\n",
    "    X_poly = poly.fit_transform(x_poly)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y)\n",
    "    \n",
    "    x_fit = np.linspace(x_poly.min(), x_poly.max(), 200).reshape(-1, 1)\n",
    "    y_fit = model.predict(poly.transform(x_fit))\n",
    "    axes[i].plot(x_fit, y_fit, color='red', linewidth=3)\n",
    "    \n",
    "    axes[i].set_xlabel(f\"IC{i+1}\", fontsize=20, fontweight='bold')\n",
    "    axes[i].set_ylabel(\"Y\", fontsize=20, fontweight='bold')\n",
    "\n",
    "# Hide unused plots if ICs < 8\n",
    "for j in range(S_.shape[1], 8):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#######################################\n",
    "\n",
    "##################################################\n",
    "# Scatter plots of each PC vs dependent variable with nonlinear fitting\n",
    "\n",
    "# Make sure X_pca exists\n",
    "df_PC = pd.DataFrame(X_pca, columns=[f\"PC{i+1}\" for i in range(X_pca.shape[1])])\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))  \n",
    "axes = axes.flatten()  \n",
    "\n",
    "for i in range(df_PC.shape[1]):\n",
    "    x = df_PC.iloc[:, i].values\n",
    "    axes[i].scatter(x, y, alpha=0.5)\n",
    "    \n",
    "    # Nonlinear polynomial fitting (degree=2)\n",
    "    x_poly = x.reshape(-1, 1)\n",
    "    poly = PolynomialFeatures(degree=2)\n",
    "    X_poly = poly.fit_transform(x_poly)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y)\n",
    "    \n",
    "    x_fit = np.linspace(x_poly.min(), x_poly.max(), 200).reshape(-1, 1)\n",
    "    y_fit = model.predict(poly.transform(x_fit))\n",
    "    axes[i].plot(x_fit, y_fit, color='red', linewidth=3)\n",
    "    \n",
    "    axes[i].set_xlabel(f\"PC{i+1}\", fontsize=12, fontweight='bold')\n",
    "    axes[i].set_ylabel(\"Concrete compressive strength\", fontsize=12, fontweight='bold')\n",
    "\n",
    "# Hide unused plots if PCs < 8\n",
    "for j in range(df_PC.shape[1], 8):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a73d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Table 4 \n",
    "##################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis, spearmanr, pearsonr\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# -------------------------------\n",
    "# 1) Load data\n",
    "# -------------------------------\n",
    "data = pd.read_csv(\"data1.csv\")\n",
    "y = data['Concrete compressive strength'].values\n",
    "feature_names = [col for col in data.columns if col != 'Concrete compressive strength']\n",
    "X = data[feature_names].values\n",
    "\n",
    "# -------------------------------\n",
    "# Function to compute indices\n",
    "# -------------------------------\n",
    "def compute_indices(Xmat, y, labels):\n",
    "    results = []\n",
    "    for i in range(Xmat.shape[1]):\n",
    "        xi = Xmat[:, i]\n",
    "\n",
    "        # Compute correlations\n",
    "        pearson_corr = pearsonr(xi, y)[0]\n",
    "        spearman_corr = spearmanr(xi, y).correlation\n",
    "\n",
    "        # Mutual Information\n",
    "        mi = mutual_info_regression(xi.reshape(-1, 1), y, random_state=0)[0]\n",
    "\n",
    "        # Simple univariate regression\n",
    "        r2_single = sm.OLS(y, sm.add_constant(xi)).fit().rsquared\n",
    "\n",
    "        results.append({\n",
    "            \"Variable\": labels[i],\n",
    "            \"Skewness\": skew(xi),\n",
    "            \"Kurtosis\": kurtosis(xi),\n",
    "            \"PearsonCorr\": pearson_corr,\n",
    "            \"SpearmanCorr\": spearman_corr,\n",
    "            \"MI\": mi,\n",
    "            \"R2_single\": r2_single\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# -------------------------------\n",
    "# 2) Indices for original data\n",
    "# -------------------------------\n",
    "df_original = compute_indices(X, y, feature_names)\n",
    "df_original[\"Type\"] = \"Original\"\n",
    "df_original[\"AbsContribution\"] = np.nan\n",
    "\n",
    "# -------------------------------\n",
    "# 3) Run PCA\n",
    "# -------------------------------\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X)\n",
    "pca_labels = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "df_pca = compute_indices(X_pca, y, pca_labels)\n",
    "df_pca[\"Type\"] = \"PCA\"\n",
    "\n",
    "# Multivariate regression for all PCs\n",
    "X_pca_const = sm.add_constant(X_pca)\n",
    "model_pcr = sm.OLS(y, X_pca_const).fit()\n",
    "coef_pcr = model_pcr.params[1:]\n",
    "share_pcr = np.abs(coef_pcr) / np.sum(np.abs(coef_pcr))\n",
    "df_pca[\"AbsContribution\"] = share_pcr\n",
    "R2_pcr = model_pcr.rsquared\n",
    "\n",
    "# -------------------------------\n",
    "# 4) Run ICA\n",
    "# -------------------------------\n",
    "ica = FastICA(random_state=0, max_iter=1000)\n",
    "X_ica = ica.fit_transform(X)\n",
    "ica_labels = [f\"IC{i+1}\" for i in range(X_ica.shape[1])]\n",
    "df_ica = compute_indices(X_ica, y, ica_labels)\n",
    "df_ica[\"Type\"] = \"ICA\"\n",
    "\n",
    "# Multivariate regression for all ICs\n",
    "X_ica_const = sm.add_constant(X_ica)\n",
    "model_icr = sm.OLS(y, X_ica_const).fit()\n",
    "coef_icr = model_icr.params[1:]\n",
    "share_icr = np.abs(coef_icr) / np.sum(np.abs(coef_icr))\n",
    "df_ica[\"AbsContribution\"] = share_icr\n",
    "R2_icr = model_icr.rsquared\n",
    "\n",
    "# -------------------------------\n",
    "# 5) Combine all tables\n",
    "# -------------------------------\n",
    "df_all = pd.concat([df_original, df_pca, df_ica], ignore_index=True)\n",
    "df_all = df_all[[\n",
    "    \"Type\", \"Variable\", \"Skewness\", \"Kurtosis\",\n",
    "    \"PearsonCorr\", \"SpearmanCorr\", \"MI\",\n",
    "    \"R2_single\", \"AbsContribution\"\n",
    "]]\n",
    "\n",
    "# -------------------------------\n",
    "# 6) Add overall R² values of models\n",
    "# -------------------------------\n",
    "summary_rows = pd.DataFrame([\n",
    "    {\"Type\": \"PCR\", \"Variable\": \"Overall\", \"R2_single\": R2_pcr},\n",
    "    {\"Type\": \"ICR\", \"Variable\": \"Overall\", \"R2_single\": R2_icr}\n",
    "])\n",
    "df_final = pd.concat([df_all, summary_rows], ignore_index=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 7) Save output\n",
    "# -------------------------------\n",
    "print(df_final)\n",
    "df_final.to_csv(\"combined_indices_full.csv\", index=False)\n",
    "df_final.to_excel(\"combined_indices_full.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f5566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear regression results for ICR and PCR  &  Figures 2,3,4 \n",
    "\n",
    "##########################################################################################################\n",
    "\n",
    "#برنامه ی نمودارهای مقاله ordered fast ica with replace t distributon instead of normal distribution:###True\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "import numpy as np\n",
    "from sklearn.decomposition import FastICA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import statsmodels.api as sm\n",
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from scipy.stats import pearsonr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy.stats import skew\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "class CustomFastICA:\n",
    "    def __init__(self, n_components, df, loc, scale, max_iter=200, tol=1e-4):\n",
    "        self.n_components = n_components\n",
    "        self.df = df  # degrees of freedom for the t-distribution\n",
    "        self.loc = loc  # mean\n",
    "        self.scale = scale  # standard deviation\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "\n",
    "    def fit(self, X):\n",
    "        n, m = X.shape\n",
    "        W = np.random.rand(self.n_components, m)\n",
    "\n",
    "        for iteration in range(self.max_iter):\n",
    "            W_old = W.copy()\n",
    "\n",
    "            # Calculate new weights\n",
    "            Y = np.dot(X, W.T)\n",
    "            g = self._tanh(Y)\n",
    "            g_dot = self._tanh_derivative(Y)\n",
    "            W = (1 / n) * np.dot(g.T, X) - np.mean(g_dot) * W\n",
    "\n",
    "            # Symmetric orthogonalization\n",
    "            W = self._symmetric_orthogonalization(W)\n",
    "\n",
    "            # Decorrelation\n",
    "            W = self._decorrelation(W)\n",
    "\n",
    "            # Check convergence\n",
    "            if iteration > 0 and np.linalg.norm(W - W_old) < self.tol:\n",
    "                break\n",
    "\n",
    "        self.components_ = W\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.dot(X, self.components_.T)\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def _tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def _tanh_derivative(self, x):\n",
    "        return 1 - np.tanh(x) ** 2\n",
    "\n",
    "    def _symmetric_orthogonalization(self, W):\n",
    "        u, s, vh = np.linalg.svd(W, full_matrices=False)\n",
    "        return np.dot(u, vh)\n",
    "\n",
    "    def _decorrelation(self, W):\n",
    "        WTW = np.dot(W, W.T)\n",
    "        _, WTW_eigenvecs = np.linalg.eigh(WTW)\n",
    "        return np.dot(WTW_eigenvecs.T, W)\n",
    "    ###########################################################################\n",
    "    \n",
    "# Replace this with your actual data\n",
    "df=pd.read_csv('data1.csv')\n",
    "X = df.values\n",
    "# Center the data\n",
    "X_centered = X - np.mean(X, axis=0)\n",
    "\n",
    "# Calculate the covariance matrix\n",
    "cov_matrix = np.cov(X_centered, rowvar=False)\n",
    "\n",
    "# Perform eigenvalue decomposition of the covariance matrix\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "# Whitening transformation\n",
    "whitening_matrix = np.dot(eigenvectors, np.dot(np.diag(1.0/np.sqrt(eigenvalues + 1e-5)), eigenvectors.T))\n",
    "\n",
    "# Whiten the data\n",
    "w_d = np.dot(X_centered, whitening_matrix)\n",
    "Tw_d = np.transpose(w_d)\n",
    "\n",
    "\n",
    "#############################################################\n",
    "s11 = df['Cement ']  # Signal 1 : sinusoidal signal\n",
    "s22 = df['Blast Furnace Slag']  # Signal 2 : square signal\n",
    "s33 = df['Fly Ash ']  \n",
    "s44=df['Water  ']\n",
    "s55=df['Superplasticize']\n",
    "s66=df['Coarse Aggregate  \\n\\n']\n",
    "s77=df['Fine Aggregate']\n",
    "s88=df['Age ']\n",
    "s99=df['Concrete compressive strength']\n",
    "ica = FastICA(n_components=8)\n",
    "A = ica.fit_transform(df)\n",
    "S = np.c_[s11, s22, s33 , s44 , s55, s66, s77, s88 ]\n",
    "X1 = np.dot(S, A.T) \n",
    "X1T= np.transpose(X1)\n",
    "\n",
    "##########################################################\n",
    "correlation_coefficients = []\n",
    "\n",
    "#for i in range(2):\n",
    "   # Generate some sample data\n",
    "s1 = Tw_d[0]  # Signal 1 : sinusoidal signal\n",
    "s2 = Tw_d[1]  # Signal 2 : square signal\n",
    "s3 = Tw_d[2] \n",
    "s4 = Tw_d[3]\n",
    "s5 = Tw_d[4]\n",
    "s6 = Tw_d[5] \n",
    "s7 = Tw_d[6] \n",
    "s8 = Tw_d[7] \n",
    "s9 = Tw_d[8] \n",
    "#ica = FastICA(n_components=8)\n",
    "#A = ica.fit_transform(Tw_d)\n",
    "\n",
    "   # Mix the signals into a single observed signal\n",
    "   #n=1030\n",
    "S = np.c_[s1, s2, s3 , s4 , s5, s6, s7, s8]\n",
    "#X1 = np.dot(S, A.T)  # Observed signal\n",
    "   \n",
    "\n",
    "\n",
    "##################\n",
    "# Center the data\n",
    "#X_centered = X - np.mean(X, axis=0)\n",
    "\n",
    "# Instantiate and fit the CustomFastICA model\n",
    "custom_ica = CustomFastICA(n_components=8, df=8.869463345651127, loc=-0.0364849978662904, scale=0.8750974313065253)\n",
    "S_ = custom_ica.fit_transform(S)\n",
    " # Fit the model \n",
    "y = df['Concrete compressive strength']\n",
    "X = np.array([S_[:, 0],S_[:, 1],S_[:, 2], S_[:, 3],S_[:, 4],S_[:, 5],S_[:, 6],S_[:, 7]])\n",
    "XT= np.transpose(X)\n",
    "XT = sm.add_constant(XT)  \n",
    "model = sm.OLS(y, XT).fit()\n",
    "# S_ now contains the independent components computed using CustomFastICA with the t-distribution\n",
    "########################################################\n",
    "\n",
    "pvalues = model.pvalues\n",
    "print(pvalues)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 4,1)\n",
    "plt.plot(X1T[0])\n",
    "plt.subplot(2, 4,2)\n",
    "plt.plot(X1T[1])\n",
    "plt.subplot(2, 4,3)\n",
    "plt.plot(X1T[2])\n",
    "plt.subplot(2, 4,4)\n",
    "plt.plot(X1T[3])\n",
    "plt.subplot(2, 4,5)\n",
    "plt.plot(X1T[4])\n",
    "plt.subplot(2, 4,6)\n",
    "plt.plot(X1T[5])\n",
    "plt.subplot(2, 4,7)\n",
    "plt.plot(X1T[6])\n",
    "plt.subplot(2, 4,8)\n",
    "plt.plot(X1T[7])\n",
    "plt.show()\n",
    "######################\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 4,1)\n",
    "plt.hist(X1T[0])\n",
    "plt.subplot(2, 4,2)\n",
    "plt.hist(X1T[1])\n",
    "plt.subplot(2, 4,3)\n",
    "plt.hist(X1T[2])\n",
    "plt.subplot(2, 4,4)\n",
    "plt.hist(X1T[3])\n",
    "plt.subplot(2, 4,5)\n",
    "plt.hist(X1T[4])\n",
    "plt.subplot(2, 4,6)\n",
    "plt.hist(X1T[5])\n",
    "plt.subplot(2, 4,7)\n",
    "plt.hist(X1T[6])\n",
    "plt.subplot(2, 4,8)\n",
    "plt.hist(X1T[7])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#######################################\n",
    "\n",
    "TS_ = np.transpose(S_)\n",
    "pvalues = model.pvalues\n",
    "plt.figure(figsize=(12, 8))\n",
    "print(pvalues)\n",
    "plt.subplot(2, 4,1)\n",
    "plt.plot(TS_[0])\n",
    "plt.subplot(2, 4,2)\n",
    "plt.plot(TS_[1])\n",
    "plt.subplot(2, 4,3)\n",
    "plt.plot(TS_[2])\n",
    "plt.subplot(2, 4,4)\n",
    "plt.plot(TS_[3])\n",
    "plt.subplot(2, 4,5)\n",
    "plt.plot(TS_[4])\n",
    "plt.subplot(2, 4,6)\n",
    "plt.plot(TS_[5])\n",
    "plt.subplot(2, 4,7)\n",
    "plt.plot(TS_[6])\n",
    "plt.subplot(2, 4,8)\n",
    "plt.plot(TS_[7])\n",
    "plt.show()\n",
    "\n",
    "#########################\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 4,1)\n",
    "plt.hist(S_[:,0])\n",
    "plt.subplot(2, 4,2)\n",
    "plt.hist(S_[:,1])\n",
    "plt.subplot(2, 4,3)\n",
    "plt.hist(S_[:,2])\n",
    "plt.subplot(2, 4,4)\n",
    "plt.hist(S_[:,3])\n",
    "plt.subplot(2, 4,5)\n",
    "plt.hist(S_[:,4])\n",
    "plt.subplot(2, 4,6)\n",
    "plt.hist(S_[:,5])\n",
    "plt.subplot(2, 4,7)\n",
    "plt.hist(S_[:,6])\n",
    "plt.subplot(2, 4,8)\n",
    "plt.hist(S_[:,7])\n",
    "plt.show()\n",
    "\n",
    "###########################\n",
    "   # Print summary \n",
    "print(model.summary())\n",
    "# Calculate the residuals\n",
    "residuals = y - model.predict(XT)\n",
    "\n",
    "# Calculate the standard deviation of the residuals\n",
    "noise_std = np.std(residuals)\n",
    "\n",
    "print(\"Noise level (standard deviation of residuals):\", noise_std)\n",
    "TS_ = np.transpose(S_)\n",
    "pvalues = model.pvalues\n",
    "\n",
    "\n",
    "   # calculate correlation coefficient\n",
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from scipy.stats import pearsonr\n",
    "   # calculate Pearson's correlation\n",
    "corr1 = pearsonr(TS_[0], s1)\n",
    "   \n",
    "corr2 = pearsonr(TS_[1], s2)\n",
    "corr3 = pearsonr(TS_[2], s3)\n",
    "corr4 = pearsonr(TS_[3], s4)\n",
    "corr5 = pearsonr(TS_[4], s5)\n",
    "corr6 = pearsonr(TS_[5], s6)\n",
    "corr7 = pearsonr(TS_[6], s7)\n",
    "corr8 = pearsonr(TS_[7], s8)\n",
    "   \n",
    "  \n",
    "\n",
    "correlation_coefficients.append([corr1, corr2, corr3, corr4,corr5,corr6,corr7,corr8])  \n",
    "   \n",
    "\n",
    "  # Assuming vector is your list of numbers\n",
    "  #X = np.array([TS_[0], TS_[1] ,TS_[2], TS_[3],TS_[4],TS_[5],TS_[6],TS_[7],TS_[8]])\n",
    "#XT= np.transpose(X)\n",
    "kurtosis_values = scipy.stats.kurtosis(S_)\n",
    "\n",
    "print(correlation_coefficients)\n",
    "print(\"Kurtosis of the vector:\", kurtosis_values)\n",
    "#print(w_d[0])\n",
    "\n",
    "# Create scatter plot\n",
    "plt.subplot(3, 3,1)\n",
    "plt.scatter(S_[:, 0], S_[:, 1])\n",
    "plt.subplot(3, 3,2)\n",
    "plt.scatter(S_[:, 1], S_[:, 2])\n",
    "plt.subplot(3, 3,3)\n",
    "plt.scatter(S_[:, 2], S_[:, 3])\n",
    "plt.subplot(3, 3,4)\n",
    "plt.scatter(S_[:, 3], S_[:, 4])\n",
    "plt.subplot(3, 3,5)\n",
    "plt.scatter(S_[:, 4], S_[:, 5])\n",
    "plt.subplot(3, 3,6)\n",
    "plt.scatter(S_[:, 5], S_[:, 6])\n",
    "plt.subplot(3, 3,7)\n",
    "plt.scatter(S_[:, 6], S_[:, 7])\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(3, 3,1)\n",
    "plt.scatter(S_[:, 0], y)\n",
    "plt.subplot(3, 3,2)\n",
    "plt.scatter(S_[:, 1], y)\n",
    "plt.subplot(3, 3,3)\n",
    "plt.scatter(S_[:, 2], y)\n",
    "plt.subplot(3, 3,4)\n",
    "plt.scatter(S_[:, 3], y)\n",
    "plt.subplot(3, 3,5)\n",
    "plt.scatter(S_[:, 4], y)\n",
    "plt.subplot(3, 3,6)\n",
    "plt.scatter(S_[:, 5], y)\n",
    "plt.subplot(3, 3,7)\n",
    "plt.scatter(S_[:, 6], y)\n",
    "plt.subplot(3, 3,8)\n",
    "plt.scatter(S_[:, 7], y)\n",
    "plt.show()\n",
    "############################################################\n",
    "import numpy as np\n",
    "\n",
    "def count_outliers(data):\n",
    "    \"\"\"\n",
    "    Count the number of outliers in a dataset using the interquartile range (IQR) method.\n",
    "\n",
    "    Parameters:\n",
    "        data (array-like): Input data (list or numpy array).\n",
    "\n",
    "    Returns:\n",
    "        num_outliers (int): Number of outliers.\n",
    "    \"\"\"\n",
    "    # Convert to numpy array if input is list\n",
    "    data = np.array(data)\n",
    "    \n",
    "    # Calculate the first and third quartiles\n",
    "    q1 = np.percentile(data, 25)\n",
    "    q3 = np.percentile(data, 75)\n",
    "\n",
    "    # Calculate the interquartile range (IQR)\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    # Define the lower and upper bounds for outliers\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "    # Count the number of outliers\n",
    "    num_outliers = np.sum((data < lower_bound) | (data > upper_bound))\n",
    "    return num_outliers\n",
    "\n",
    "# Example usage:\n",
    "data = S_\n",
    "num_outliers = count_outliers(data)\n",
    "print(\"Number of outliers:\", num_outliers)\n",
    "\n",
    "\n",
    "###################################################\n",
    "\n",
    "# Show the plot\n",
    "#plt.xlabel('Third Principal Component')\n",
    "#plt.ylabel('Sixth Principal Component')\n",
    "#plt.title('Scatter Plot of Third vs Sixth Principal Components')\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "############ correlation between y and S_  for noise:\n",
    "corr11 = pearsonr(TS_[0], y)\n",
    "   \n",
    "corr12 = pearsonr(TS_[1], y)\n",
    "corr13 = pearsonr(TS_[2], y)\n",
    "corr14 = pearsonr(TS_[3], y)\n",
    "corr15 = pearsonr(TS_[4], y)\n",
    "corr16 = pearsonr(TS_[5], y)\n",
    "corr17 = pearsonr(TS_[6], y)\n",
    "corr18 = pearsonr(TS_[7], y)\n",
    "   \n",
    "  \n",
    "\n",
    "correlation_coefficients.append([corr11, corr12, corr13, corr14,corr15,corr16,corr17,corr18])  \n",
    "   \n",
    "\n",
    "  # Assuming vector is your list of numbers\n",
    "  #X = np.array([TS_[0], TS_[1] ,TS_[2], TS_[3],TS_[4],TS_[5],TS_[6],TS_[7],TS_[8]])\n",
    "#XT= np.transpose(X)\n",
    "\n",
    "print(correlation_coefficients)\n",
    "\n",
    "skewness_values = scipy.stats.skew(S_, axis=0)\n",
    "print(\"Skewness of the independent components:\")\n",
    "print(skewness_values)\n",
    "\n",
    "\n",
    "########################3\n",
    "#qqplot for cosider symmetric of S_\n",
    "\n",
    "# Calculate the residuals\n",
    "residuals = y - model.predict(XT)\n",
    "\n",
    "# Generate Q-Q plots for the residuals\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for i in range(8):\n",
    "    ax = axes[i // 4, i % 4]\n",
    "    qqplot(residuals, line='s', ax=ax)\n",
    "    ax.set_title(f\"Residuals Q-Q Plot for Component {i+1}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "##########################################\n",
    "#Anova for realize nonlinear relationship between y and S_:\n",
    "\n",
    "# Generate polynomial features for the independent variables\n",
    "poly = PolynomialFeatures(degree=2)  # You can adjust the degree as needed\n",
    "S_poly = poly.fit_transform(S_)\n",
    "\n",
    "# Fit the ANOVA model with polynomial terms\n",
    "model_anova = sm.OLS(y, S_poly).fit()\n",
    "\n",
    "# Print summary of ANOVA model\n",
    "print(model_anova.summary())\n",
    "\n",
    "\n",
    "##################################################\n",
    "# plot of residual against of predicted value:\n",
    "#####################################\n",
    "# Assuming S_ contains the independent components computed using CustomFastICA with the t-distribution\n",
    "\n",
    "# Reshape the first column of S_ to a 2D array with a single feature\n",
    "X_first_column = S_[:, 0].reshape(-1, 1)\n",
    "X_two_column = S_[:, 1].reshape(-1, 1)\n",
    "X_three_column = S_[:, 2].reshape(-1, 1)\n",
    "X_four_column = S_[:, 3].reshape(-1, 1)\n",
    "X_five_column = S_[:, 4].reshape(-1, 1)\n",
    "X_six_column = S_[:, 5].reshape(-1, 1)\n",
    "X_seven_column = S_[:, 6].reshape(-1, 1)\n",
    "X_eight_column = S_[:, 7].reshape(-1, 1)\n",
    "\n",
    "# Fit a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_first_column, y)\n",
    "model.fit(X_two_column, y)\n",
    "model.fit(X_three_column, y)\n",
    "model.fit(X_four_column, y)\n",
    "model.fit(X_five_column , y)\n",
    "model.fit(X_six_column, y)\n",
    "model.fit(X_seven_column, y)\n",
    "model.fit(X_eight_column, y)\n",
    "\n",
    "# Calculate residuals\n",
    "residuals1 = y - model.predict(X_first_column)\n",
    "residuals2 = y - model.predict(X_two_column)\n",
    "residuals3 = y - model.predict(X_three_column)\n",
    "residuals4 = y - model.predict(X_four_column)\n",
    "residuals5 = y - model.predict(X_five_column )\n",
    "residuals6 = y - model.predict(X_six_column)\n",
    "residuals7 = y - model.predict(X_seven_column)\n",
    "residuals8= y - model.predict(X_eight_column)\n",
    "\n",
    "# Display the model coefficients\n",
    "print(\"Model Coefficients:\", model.coef_)\n",
    "\n",
    "# Display the residuals\n",
    "#print(\"Residuals:\")\n",
    "#for value in residuals:\n",
    "#    print(value)\n",
    "    \n",
    "# Plot residuals vs. predicted values\n",
    "predicted_values1 = model.predict(X_first_column)\n",
    "predicted_values2 = model.predict(X_two_column)\n",
    "predicted_values3 = model.predict(X_three_column)\n",
    "predicted_values4 = model.predict(X_four_column)\n",
    "predicted_values5 = model.predict(X_five_column)\n",
    "predicted_values6 = model.predict(X_six_column)\n",
    "predicted_values7 = model.predict(X_seven_column)\n",
    "predicted_values8 = model.predict(X_eight_column)\n",
    "\n",
    "plt.subplot(3, 3,1)\n",
    "plt.scatter(predicted_values1, residuals1)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "#plt.title(\"Residual Plot\")\n",
    "plt.axhline(y=0, color='r', linestyle='-')  # Add horizontal line at y=0\n",
    "\n",
    "plt.subplot(3, 3,2)\n",
    "plt.scatter(predicted_values2, residuals2)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "#plt.title(\"Residual Plot\")\n",
    "plt.axhline(y=0, color='r', linestyle='-')  # Add horizontal line at y=0\n",
    "\n",
    "plt.subplot(3, 3,3)\n",
    "plt.scatter(predicted_values3, residuals3)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "#plt.title(\"Residual Plot\")\n",
    "plt.axhline(y=0, color='r', linestyle='-')  # Add horizontal line at y=0\n",
    "\n",
    "plt.subplot(3, 3,4)\n",
    "plt.scatter(predicted_values4, residuals4)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "#plt.title(\"Residual Plot\")\n",
    "plt.axhline(y=0, color='r', linestyle='-')  # Add horizontal line at y=0\n",
    "\n",
    "plt.subplot(3, 3,5)\n",
    "plt.scatter(predicted_values5, residuals5)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "#plt.title(\"Residual Plot\")\n",
    "plt.axhline(y=0, color='r', linestyle='-')  # Add horizontal line at y=0\n",
    "\n",
    "plt.subplot(3, 3,6)\n",
    "plt.scatter(predicted_values6, residuals6)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "#plt.title(\"Residual Plot\")\n",
    "plt.axhline(y=0, color='r', linestyle='-')  # Add horizontal line at y=0\n",
    "\n",
    "plt.subplot(3, 3,7)\n",
    "plt.scatter(predicted_values7, residuals7)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "#plt.title(\"Residual Plot\")\n",
    "plt.axhline(y=0, color='r', linestyle='-')  # Add horizontal line at y=0\n",
    "\n",
    "plt.subplot(3, 3,8)\n",
    "plt.scatter(predicted_values8, residuals8)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "#plt.title(\"Residual Plot\")\n",
    "plt.axhline(y=0, color='r', linestyle='-')  # Add horizontal line at y=0\n",
    "plt.show()\n",
    "\n",
    "### تشخیص هم خطی\n",
    "  # calculate correlation coefficient\n",
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from scipy.stats import pearsonr\n",
    "# calculate Pearson's correlation\n",
    "corr1 = pearsonr(TS_[0], df['Cement '])\n",
    "corr11 = pearsonr(TS_[0],df['Blast Furnace Slag'] )\n",
    "corr12 = pearsonr(TS_[0],df['Fly Ash '] )\n",
    "corr13 = pearsonr(TS_[0], df['Water  '])\n",
    "corr14 = pearsonr(TS_[0], df['Superplasticize'])\n",
    "corr15 = pearsonr(TS_[0], df['Coarse Aggregate  \\n\\n'])\n",
    "corr16 = pearsonr(TS_[0],df['Fine Aggregate'])\n",
    "corr17 = pearsonr(TS_[0], df['Age '])\n",
    "#corr18 = pearsonr(TS_[0], 'Concrete compressive strength')\n",
    "#corr2 = pearsonr(TS_[1], s1)\n",
    "#corr21 = pearsonr(TS_[1], s2)\n",
    "#corr22 = pearsonr(TS_[1], s3)\n",
    "#corr23 = pearsonr(TS_[1], s4)\n",
    "#corr24 = pearsonr(TS_[1], s5)\n",
    "#corr25 = pearsonr(TS_[1], s6)\n",
    "#corr26 = pearsonr(TS_[1], s7)\n",
    "#corr27 = pearsonr(TS_[1], s8)\n",
    "#corr28 = pearsonr(TS_[1], s9)\n",
    "#corr3 = pearsonr(TS_[2], s1)\n",
    "#corr31 = pearsonr(TS_[2], s2)\n",
    "#corr32 = pearsonr(TS_[2], s3)\n",
    "#corr33 = pearsonr(TS_[2], s4) \n",
    "#corr34 = pearsonr(TS_[2], s5)\n",
    "#corr35 = pearsonr(TS_[2], s6)\n",
    "#corr36 = pearsonr(TS_[2], s7) \n",
    "#corr37 = pearsonr(TS_[2], s8)\n",
    "#corr38 = pearsonr(TS_[2], s9) \n",
    "#corr4 = pearsonr(TS_[3], s1)\n",
    "#corr41 = pearsonr(TS_[3], s2) \n",
    "#corr42 = pearsonr(TS_[3], s3) \n",
    "#corr43 = pearsonr(TS_[3], s4) \n",
    "#corr44 = pearsonr(TS_[3], s5) \n",
    "#corr45 = pearsonr(TS_[3], s6) \n",
    "#corr46 = pearsonr(TS_[3], s7)\n",
    "#corr47 = pearsonr(TS_[3], s8) \n",
    "#corr48 = pearsonr(TS_[3], s9)\n",
    "#corr5 = pearsonr(TS_[4], s1)\n",
    "#corr51 = pearsonr(TS_[4], s2)\n",
    "#corr52 = pearsonr(TS_[4], s3)\n",
    "#corr53 = pearsonr(TS_[4], s4)\n",
    "#corr54 = pearsonr(TS_[4], s5)\n",
    "#corr55 = pearsonr(TS_[4], s6)\n",
    "#corr56 = pearsonr(TS_[4], s7)\n",
    "#corr57 = pearsonr(TS_[4], s8)\n",
    "#corr58 = pearsonr(TS_[4], s9)\n",
    "#corr6 = pearsonr(TS_[5], s1)\n",
    "#corr61 = pearsonr(TS_[5], s2)\n",
    "#corr62= pearsonr(TS_[5], s3)\n",
    "#corr63= pearsonr(TS_[5], s4)\n",
    "#corr64= pearsonr(TS_[5], s5)\n",
    "##corr65= pearsonr(TS_[5], s6)\n",
    "#corr66= pearsonr(TS_[5], s7)\n",
    "#corr67= pearsonr(TS_[5], s8)\n",
    "#corr68= pearsonr(TS_[5], s9)\n",
    "#corr7 = pearsonr(TS_[6], s1)\n",
    "#corr71 = pearsonr(TS_[6], s2)\n",
    "#corr72 = pearsonr(TS_[6], s3)\n",
    "#corr73= pearsonr(TS_[6], s4)\n",
    "#corr74 = pearsonr(TS_[6], s5)\n",
    "#corr75 = pearsonr(TS_[6], s6)\n",
    "#corr76 = pearsonr(TS_[6], s7)\n",
    "#corr77 = pearsonr(TS_[6], s8)\n",
    "#corr78 = pearsonr(TS_[6], s9)\n",
    "#corr8 = pearsonr(TS_[7], s1)\n",
    "#corr81 = pearsonr(TS_[7], s2)\n",
    "#corr82 = pearsonr(TS_[7], s3)\n",
    "#corr83 = pearsonr(TS_[7], s4)\n",
    "#corr84 = pearsonr(TS_[7], s5)\n",
    "#corr85 = pearsonr(TS_[7], s6)\n",
    "#corr86 = pearsonr(TS_[7], s7)\n",
    "#corr87 = pearsonr(TS_[7], s8)\n",
    "#corr88 = pearsonr(TS_[7], s9)\n",
    "#corr9 = pearsonr(TS_[8], s1)\n",
    "#corr91 = pearsonr(TS_[8], s2)\n",
    "#corr92= pearsonr(TS_[8], s3)\n",
    "#corr93 = pearsonr(TS_[8], s4)\n",
    "#corr94 = pearsonr(TS_[8], s5)\n",
    "#corr95 = pearsonr(TS_[8], s6)\n",
    "#corr96 = pearsonr(TS_[8], s7)\n",
    "#corr97 = pearsonr(TS_[8], s8)\n",
    "#corr98 = pearsonr(TS_[8], s9)\n",
    "\n",
    "correlation_coefficients.append([corr1,corr11,corr12,corr13,corr14,corr15,corr16,corr17])#, corr2,corr21,corr22,corr23,corr24,corr25,corr26,corr27,corr3,corr31,corr32,corr33,corr34,corr35, corr36, corr37, corr4,corr41,corr42,corr43,corr44,corr45,corr46,corr47,corr5,corr51,corr52,corr53,corr54,corr55,corr56,corr57, corr6,corr61,corr62,corr63,corr64,corr65,corr66,corr67,corr7,corr71,corr72,corr73,corr74,corr75,corr76,corr77,corr8, corr81,corr82,corr83,corr84,corr85,corr86,corr87]) \n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(3, 3,1)\n",
    "plt.hist(s11)\n",
    "plt.subplot(3, 3,2)\n",
    "plt.hist(s22)\n",
    "plt.subplot(3, 3,3)\n",
    "plt.hist(s33)\n",
    "plt.subplot(3, 3,4)\n",
    "plt.hist(s44)\n",
    "plt.subplot(3, 3,5)\n",
    "plt.hist(s55)\n",
    "plt.subplot(3, 3,6)\n",
    "plt.hist(s66)\n",
    "plt.subplot(3, 3,7)\n",
    "plt.hist(s77)\n",
    "plt.subplot(3, 3,8)\n",
    "plt.hist(s88)\n",
    "#plt.subplot(3, 3,9)\n",
    "#plt.hist(s99)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c42dcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concrete data, Results for Lasso regression: \n",
    "\n",
    "# ordered fast ica with replace t distributon instead on normal distribution.... Lasso Regression   :###True\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "import numpy as np\n",
    "from sklearn.decomposition import FastICA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import statsmodels.api as sm\n",
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from scipy.stats import pearsonr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy.stats import skew\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "class CustomFastICA:\n",
    "    def __init__(self, n_components, df, loc, scale, max_iter=200, tol=1e-4):\n",
    "        self.n_components = n_components\n",
    "        self.df = df  # degrees of freedom for the t-distribution\n",
    "        self.loc = loc  # mean\n",
    "        self.scale = scale  # standard deviation\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "\n",
    "    def fit(self, X):\n",
    "        n, m = X.shape\n",
    "        W = np.random.rand(self.n_components, m)\n",
    "\n",
    "        for iteration in range(self.max_iter):\n",
    "            W_old = W.copy()\n",
    "\n",
    "            # Calculate new weights\n",
    "            Y = np.dot(X, W.T)\n",
    "            g = self._tanh(Y)\n",
    "            g_dot = self._tanh_derivative(Y)\n",
    "            W = (1 / n) * np.dot(g.T, X) - np.mean(g_dot) * W\n",
    "\n",
    "            # Symmetric orthogonalization\n",
    "            W = self._symmetric_orthogonalization(W)\n",
    "\n",
    "            # Decorrelation\n",
    "            W = self._decorrelation(W)\n",
    "\n",
    "            # Check convergence\n",
    "            if iteration > 0 and np.linalg.norm(W - W_old) < self.tol:\n",
    "                break\n",
    "\n",
    "        self.components_ = W\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.dot(X, self.components_.T)\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def _tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def _tanh_derivative(self, x):\n",
    "        return 1 - np.tanh(x) ** 2\n",
    "\n",
    "    def _symmetric_orthogonalization(self, W):\n",
    "        u, s, vh = np.linalg.svd(W, full_matrices=False)\n",
    "        return np.dot(u, vh)\n",
    "\n",
    "    def _decorrelation(self, W):\n",
    "        WTW = np.dot(W, W.T)\n",
    "        _, WTW_eigenvecs = np.linalg.eigh(WTW)\n",
    "        return np.dot(WTW_eigenvecs.T, W)\n",
    "    ###########################################################################\n",
    "    \n",
    "# Replace this with your actual data\n",
    "df=pd.read_csv('data1.csv')\n",
    "X = df.values\n",
    "# Center the data\n",
    "X_centered = X - np.mean(X, axis=0)\n",
    "\n",
    "# Calculate the covariance matrix\n",
    "cov_matrix = np.cov(X_centered, rowvar=False)\n",
    "\n",
    "# Perform eigenvalue decomposition of the covariance matrix\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "# Whitening transformation\n",
    "whitening_matrix = np.dot(eigenvectors, np.dot(np.diag(1.0/np.sqrt(eigenvalues + 1e-5)), eigenvectors.T))\n",
    "\n",
    "# Whiten the data\n",
    "w_d = np.dot(X_centered, whitening_matrix)\n",
    "Tw_d = np.transpose(w_d)\n",
    "\n",
    "\n",
    "correlation_coefficients = []\n",
    "\n",
    "#for i in range(2):\n",
    "   # Generate some sample data\n",
    "s1 = Tw_d[0]  # Signal 1 : sinusoidal signal\n",
    "s2 = Tw_d[1]  # Signal 2 : square signal\n",
    "s3 = Tw_d[2] \n",
    "s4 = Tw_d[3]\n",
    "s5 = Tw_d[4]\n",
    "s6 = Tw_d[5] \n",
    "s7 = Tw_d[6] \n",
    "s8 = Tw_d[7] \n",
    "s9 = Tw_d[8] \n",
    "#ica = FastICA(n_components=8)\n",
    "#A = ica.fit_transform(Tw_d)\n",
    "\n",
    "   # Mix the signals into a single observed signal\n",
    "   #n=1030\n",
    "S = np.c_[s1, s2, s3 , s4 , s5, s6, s7, s8]\n",
    "#X1 = np.dot(S, A.T)  # Observed signal\n",
    "   \n",
    "\n",
    "\n",
    "##################\n",
    "# Center the data\n",
    "#X_centered = X - np.mean(X, axis=0)\n",
    "\n",
    "# Instantiate and fit the CustomFastICA model\n",
    "custom_ica = CustomFastICA(n_components=8, df=8.869463345651127, loc=-0.0364849978662904, scale=0.8750974313065253)\n",
    "S_ = custom_ica.fit_transform(S)\n",
    " # Fit the model \n",
    "y = df['Concrete compressive strength']\n",
    "X = np.array([S_[:, 0],S_[:, 1],S_[:, 2], S_[:, 3],S_[:, 4],S_[:, 5],S_[:, 6],S_[:, 7]])\n",
    "XT= np.transpose(X)\n",
    "XT = sm.add_constant(XT)  \n",
    "# S_ now contains the independent components computed using CustomFastICA with the t-distribution\n",
    "\n",
    "###################################################\n",
    "# Fit the model using Lasso regression\n",
    "alpha = 1.0  # You can adjust the regularization strength as needed\n",
    "lasso_model = Lasso(alpha=alpha)\n",
    "lasso_model.fit(XT, y)\n",
    "\n",
    "# Calculate predicted values\n",
    "y_pred_lasso = lasso_model.predict(XT)\n",
    "\n",
    "# Calculate R-squared manually for Lasso regression\n",
    "ss_residual_lasso = np.sum((y - y_pred_lasso) ** 2)\n",
    "r_squared_lasso = 1 - (ss_residual_lasso / ss_total)\n",
    "print(\"R-squared (Lasso):\", r_squared_lasso)\n",
    "\n",
    "# Print intercept and coefficients for Lasso regression\n",
    "print(\"Intercept (Lasso):\", lasso_model.intercept_)\n",
    "print(\"Coefficients (Lasso):\", lasso_model.coef_)\n",
    "\n",
    "########################################################\n",
    "\n",
    "TS_ = np.transpose(S_)\n",
    "#pvalues = ridge_model.pvalues\n",
    "#print(pvalues)\n",
    "plt.subplot(3, 3,1)\n",
    "plt.plot(TS_[0])\n",
    "plt.subplot(3, 3,2)\n",
    "plt.plot(TS_[1])\n",
    "plt.subplot(3, 3,3)\n",
    "plt.plot(TS_[2])\n",
    "plt.subplot(3, 3,4)\n",
    "plt.plot(TS_[3])\n",
    "plt.subplot(3, 3,5)\n",
    "plt.plot(TS_[4])\n",
    "plt.subplot(3, 3,6)\n",
    "plt.plot(TS_[5])\n",
    "plt.subplot(3, 3,7)\n",
    "plt.plot(TS_[6])\n",
    "plt.subplot(3, 3,8)\n",
    "plt.plot(TS_[7])\n",
    "plt.show()\n",
    "\n",
    "   # Print summary \n",
    "\n",
    "# Calculate the residuals\n",
    "residuals = y - lasso_model.predict(XT)\n",
    "\n",
    "# Calculate the standard deviation of the residuals\n",
    "noise_std = np.std(residuals)\n",
    "\n",
    "print(\"Noise level (standard deviation of residuals):\", noise_std)\n",
    "TS_ = np.transpose(S_)\n",
    "#pvalues = ridge_model.pvalues\n",
    "plt.subplot(3, 3,1)\n",
    "plt.hist(TS_[0])\n",
    "plt.subplot(3, 3,2)\n",
    "plt.hist(TS_[1])\n",
    "plt.subplot(3, 3,3)\n",
    "plt.hist(TS_[2])\n",
    "plt.subplot(3, 3,4)\n",
    "plt.hist(TS_[3])\n",
    "plt.subplot(3, 3,5)\n",
    "plt.hist(TS_[4])\n",
    "plt.subplot(3, 3,6)\n",
    "plt.hist(TS_[5])\n",
    "plt.subplot(3, 3,7)\n",
    "plt.hist(TS_[6])\n",
    "plt.subplot(3, 3,8)\n",
    "plt.hist(TS_[7])\n",
    "plt.show()\n",
    "\n",
    "   # calculate correlation coefficient\n",
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from scipy.stats import pearsonr\n",
    "   # calculate Pearson's correlation\n",
    "corr1 = pearsonr(TS_[0], s1)\n",
    "   \n",
    "corr2 = pearsonr(TS_[1], s2)\n",
    "corr3 = pearsonr(TS_[2], s3)\n",
    "corr4 = pearsonr(TS_[3], s4)\n",
    "corr5 = pearsonr(TS_[4], s5)\n",
    "corr6 = pearsonr(TS_[5], s6)\n",
    "corr7 = pearsonr(TS_[6], s7)\n",
    "corr8 = pearsonr(TS_[7], s8)\n",
    "   \n",
    "  \n",
    "\n",
    "correlation_coefficients.append([corr1, corr2, corr3, corr4,corr5,corr6,corr7,corr8])  \n",
    "   \n",
    "\n",
    "  # Assuming vector is your list of numbers\n",
    "  #X = np.array([TS_[0], TS_[1] ,TS_[2], TS_[3],TS_[4],TS_[5],TS_[6],TS_[7],TS_[8]])\n",
    "#XT= np.transpose(X)\n",
    "kurtosis_values = scipy.stats.kurtosis(S_)\n",
    "\n",
    "print(correlation_coefficients)\n",
    "print(\"Kurtosis of the vector:\", kurtosis_values)\n",
    "#print(w_d[0])\n",
    "\n",
    "# Create scatter plot\n",
    "plt.subplot(3, 3,1)\n",
    "plt.scatter(S_[:, 0], S_[:, 1])\n",
    "plt.subplot(3, 3,2)\n",
    "plt.scatter(S_[:, 1], S_[:, 2])\n",
    "plt.subplot(3, 3,3)\n",
    "plt.scatter(S_[:, 2], S_[:, 3])\n",
    "plt.subplot(3, 3,4)\n",
    "plt.scatter(S_[:, 3], S_[:, 4])\n",
    "plt.subplot(3, 3,5)\n",
    "plt.scatter(S_[:, 4], S_[:, 5])\n",
    "plt.subplot(3, 3,6)\n",
    "plt.scatter(S_[:, 5], S_[:, 6])\n",
    "plt.subplot(3, 3,7)\n",
    "plt.scatter(S_[:, 6], S_[:, 7])\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(3, 3,1)\n",
    "plt.scatter(S_[:, 0], y)\n",
    "plt.subplot(3, 3,2)\n",
    "plt.scatter(S_[:, 1], y)\n",
    "plt.subplot(3, 3,3)\n",
    "plt.scatter(S_[:, 2], y)\n",
    "plt.subplot(3, 3,4)\n",
    "plt.scatter(S_[:, 3], y)\n",
    "plt.subplot(3, 3,5)\n",
    "plt.scatter(S_[:, 4], y)\n",
    "plt.subplot(3, 3,6)\n",
    "plt.scatter(S_[:, 5], y)\n",
    "plt.subplot(3, 3,7)\n",
    "plt.scatter(S_[:, 6], y)\n",
    "plt.subplot(3, 3,8)\n",
    "plt.scatter(S_[:, 7], y)\n",
    "plt.show()\n",
    "############################################################\n",
    "import numpy as np\n",
    "\n",
    "def count_outliers(data):\n",
    "    \"\"\"\n",
    "    Count the number of outliers in a dataset using the interquartile range (IQR) method.\n",
    "\n",
    "    Parameters:\n",
    "        data (array-like): Input data (list or numpy array).\n",
    "\n",
    "    Returns:\n",
    "        num_outliers (int): Number of outliers.\n",
    "    \"\"\"\n",
    "    # Convert to numpy array if input is list\n",
    "    data = np.array(data)\n",
    "    \n",
    "    # Calculate the first and third quartiles\n",
    "    q1 = np.percentile(data, 25)\n",
    "    q3 = np.percentile(data, 75)\n",
    "\n",
    "    # Calculate the interquartile range (IQR)\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    # Define the lower and upper bounds for outliers\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "    # Count the number of outliers\n",
    "    num_outliers = np.sum((data < lower_bound) | (data > upper_bound))\n",
    "    return num_outliers\n",
    "\n",
    "# Example usage:\n",
    "data = S_\n",
    "num_outliers = count_outliers(data)\n",
    "print(\"Number of outliers:\", num_outliers)\n",
    "\n",
    "\n",
    "###################################################\n",
    "\n",
    "# Show the plot\n",
    "#plt.xlabel('Third Principal Component')\n",
    "#plt.ylabel('Sixth Principal Component')\n",
    "#plt.title('Scatter Plot of Third vs Sixth Principal Components')\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "############ correlation between y and S_  for noise:\n",
    "corr11 = pearsonr(TS_[0], y)\n",
    "   \n",
    "corr12 = pearsonr(TS_[1], y)\n",
    "corr13 = pearsonr(TS_[2], y)\n",
    "corr14 = pearsonr(TS_[3], y)\n",
    "corr15 = pearsonr(TS_[4], y)\n",
    "corr16 = pearsonr(TS_[5], y)\n",
    "corr17 = pearsonr(TS_[6], y)\n",
    "corr18 = pearsonr(TS_[7], y)\n",
    "   \n",
    "  \n",
    "\n",
    "correlation_coefficients.append([corr11, corr12, corr13, corr14,corr15,corr16,corr17,corr18])  \n",
    "   \n",
    "\n",
    "  # Assuming vector is your list of numbers\n",
    "  #X = np.array([TS_[0], TS_[1] ,TS_[2], TS_[3],TS_[4],TS_[5],TS_[6],TS_[7],TS_[8]])\n",
    "#XT= np.transpose(X)\n",
    "\n",
    "print(correlation_coefficients)\n",
    "\n",
    "skewness_values = scipy.stats.skew(S_, axis=0)\n",
    "print(\"Skewness of the independent components:\")\n",
    "print(skewness_values)\n",
    "\n",
    "\n",
    "########################3\n",
    "#qqplot for cosider symmetric of S_\n",
    "\n",
    "# Calculate the residuals\n",
    "residuals = y -  lasso_model.predict(XT)\n",
    "\n",
    "# Generate Q-Q plots for the residuals\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for i in range(8):\n",
    "    ax = axes[i // 4, i % 4]\n",
    "    qqplot(residuals, line='s', ax=ax)\n",
    "    ax.set_title(f\"Residuals Q-Q Plot for Component {i+1}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "##########################################\n",
    "#Anova for realize nonlinear relationship between y and S_:\n",
    "\n",
    "# Generate polynomial features for the independent variables\n",
    "poly = PolynomialFeatures(degree=2)  # You can adjust the degree as needed\n",
    "S_poly = poly.fit_transform(S_)\n",
    "\n",
    "# Fit the ANOVA model with polynomial terms\n",
    "model_anova = sm.OLS(y, S_poly).fit()\n",
    "\n",
    "# Print summary of ANOVA model\n",
    "print(model_anova.summary())\n",
    "\n",
    "\n",
    "##################################################\n",
    "# plot of residual against of predicted value:\n",
    "#####################################\n",
    "# Assuming S_ contains the independent components computed using CustomFastICA with the t-distribution\n",
    "\n",
    "# Reshape the first column of S_ to a 2D array with a single feature\n",
    "X_first_column = S_[:, 0].reshape(-1, 1)\n",
    "X_two_column = S_[:, 1].reshape(-1, 1)\n",
    "X_three_column = S_[:, 2].reshape(-1, 1)\n",
    "X_four_column = S_[:, 3].reshape(-1, 1)\n",
    "X_five_column = S_[:, 4].reshape(-1, 1)\n",
    "X_six_column = S_[:, 5].reshape(-1, 1)\n",
    "X_seven_column = S_[:, 6].reshape(-1, 1)\n",
    "X_eight_column = S_[:, 7].reshape(-1, 1)\n",
    "\n",
    "# Fit a linear regression model\n",
    "#model = LinearRegression()\n",
    "lasso_model.fit(X_first_column, y)\n",
    "lasso_model.fit(X_two_column, y)\n",
    "lasso_model.fit(X_three_column, y)\n",
    "lasso_model.fit(X_four_column, y)\n",
    "lasso_model.fit(X_five_column , y)\n",
    "lasso_model.fit(X_six_column, y)\n",
    "lasso_model.fit(X_seven_column, y)\n",
    "lasso_model.fit(X_eight_column, y)\n",
    "\n",
    "# Calculate residuals\n",
    "residuals1 = y - lasso_model.predict(X_first_column)\n",
    "residuals2 = y - lasso_model.predict(X_two_column)\n",
    "residuals3 = y - lasso_model.predict(X_three_column)\n",
    "residuals4 = y - lasso_model.predict(X_four_column)\n",
    "residuals5 = y - lasso_model.predict(X_five_column )\n",
    "residuals6 = y - lasso_model.predict(X_six_column)\n",
    "residuals7 = y - lasso_model.predict(X_seven_column)\n",
    "residuals8= y - lasso_model.predict(X_eight_column)\n",
    "\n",
    "# Display the model coefficients\n",
    "print(\"Model Coefficients:\", lasso_model.coef_)\n",
    "\n",
    "# Display the residuals\n",
    "#print(\"Residuals:\")\n",
    "#for value in residuals:\n",
    "#    print(value)\n",
    "    \n",
    "# Plot residuals vs. predicted values\n",
    "predicted_values1 = lasso_model.predict(X_first_column)\n",
    "predicted_values2 = lasso_model.predict(X_two_column)\n",
    "predicted_values3 = lasso_model.predict(X_three_column)\n",
    "predicted_values4 = lasso_model.predict(X_four_column)\n",
    "predicted_values5 = lasso_model.predict(X_five_column)\n",
    "predicted_values6 = lasso_model.predict(X_six_column)\n",
    "predicted_values7 = lasso_model.predict(X_seven_column)\n",
    "predicted_values8 = lasso_model.predict(X_eight_column)\n",
    "\n",
    "plt.subplot(3, 3,1)\n",
    "plt.scatter(predicted_values1, residuals1)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "#plt.title(\"Residual Plot\")\n",
    "plt.axhline(y=0, color='r', linestyle='-')  # Add horizontal line at y=0\n",
    "\n",
    "plt.subplot(3, 3,2)\n",
    "plt.scatter(predicted_values2, residuals2)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "#plt.title(\"Residual Plot\")\n",
    "plt.axhline(y=0, color='r', linestyle='-')  # Add horizontal line at y=0\n",
    "\n",
    "plt.subplot(3, 3,3)\n",
    "plt.scatter(predicted_values3, residuals3)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "#plt.title(\"Residual Plot\")\n",
    "plt.axhline(y=0, color='r', linestyle='-')  # Add horizontal line at y=0\n",
    "\n",
    "plt.subplot(3, 3,4)\n",
    "plt.scatter(predicted_values4, residuals4)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "#plt.title(\"Residual Plot\")\n",
    "plt.axhline(y=0, color='r', linestyle='-')  # Add horizontal line at y=0\n",
    "\n",
    "plt.subplot(3, 3,5)\n",
    "plt.scatter(predicted_values5, residuals5)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "#plt.title(\"Residual Plot\")\n",
    "plt.axhline(y=0, color='r', linestyle='-')  # Add horizontal line at y=0\n",
    "\n",
    "plt.subplot(3, 3,6)\n",
    "plt.scatter(predicted_values6, residuals6)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "#plt.title(\"Residual Plot\")\n",
    "plt.axhline(y=0, color='r', linestyle='-')  # Add horizontal line at y=0\n",
    "\n",
    "plt.subplot(3, 3,7)\n",
    "plt.scatter(predicted_values7, residuals7)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "#plt.title(\"Residual Plot\")\n",
    "plt.axhline(y=0, color='r', linestyle='-')  # Add horizontal line at y=0\n",
    "\n",
    "plt.subplot(3, 3,8)\n",
    "plt.scatter(predicted_values8, residuals8)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "#plt.title(\"Residual Plot\")\n",
    "plt.axhline(y=0, color='r', linestyle='-')  # Add horizontal line at y=0\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bc3af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## concrete data ## results for ridge regression\n",
    "\n",
    "##########################################################################\n",
    " \n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "import numpy as np\n",
    "from sklearn.decomposition import FastICA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import statsmodels.api as sm\n",
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from scipy.stats import pearsonr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy.stats import skew\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "class CustomFastICA:\n",
    "    def __init__(self, n_components, df, loc, scale, max_iter=200, tol=1e-4):\n",
    "        self.n_components = n_components\n",
    "        self.df = df  # degrees of freedom for the t-distribution\n",
    "        self.loc = loc  # mean\n",
    "        self.scale = scale  # standard deviation\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "\n",
    "    def fit(self, X):\n",
    "        n, m = X.shape\n",
    "        W = np.random.rand(self.n_components, m)\n",
    "\n",
    "        for iteration in range(self.max_iter):\n",
    "            W_old = W.copy()\n",
    "\n",
    "            # Calculate new weights\n",
    "            Y = np.dot(X, W.T)\n",
    "            g = self._tanh(Y)\n",
    "            g_dot = self._tanh_derivative(Y)\n",
    "            W = (1 / n) * np.dot(g.T, X) - np.mean(g_dot) * W\n",
    "\n",
    "            # Symmetric orthogonalization\n",
    "            W = self._symmetric_orthogonalization(W)\n",
    "\n",
    "            # Decorrelation\n",
    "            W = self._decorrelation(W)\n",
    "\n",
    "            # Check convergence\n",
    "            if iteration > 0 and np.linalg.norm(W - W_old) < self.tol:\n",
    "                break\n",
    "\n",
    "        self.components_ = W\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.dot(X, self.components_.T)\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def _tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def _tanh_derivative(self, x):\n",
    "        return 1 - np.tanh(x) ** 2\n",
    "\n",
    "    def _symmetric_orthogonalization(self, W):\n",
    "        u, s, vh = np.linalg.svd(W, full_matrices=False)\n",
    "        return np.dot(u, vh)\n",
    "\n",
    "    def _decorrelation(self, W):\n",
    "        WTW = np.dot(W, W.T)\n",
    "        _, WTW_eigenvecs = np.linalg.eigh(WTW)\n",
    "        return np.dot(WTW_eigenvecs.T, W)\n",
    "    ###########################################################################\n",
    "    \n",
    "# Replace this with your actual data\n",
    "df=pd.read_csv('data1.csv')\n",
    "X = df.values\n",
    "# Center the data\n",
    "X_centered = X - np.mean(X, axis=0)\n",
    "\n",
    "# Calculate the covariance matrix\n",
    "cov_matrix = np.cov(X_centered, rowvar=False)\n",
    "\n",
    "# Perform eigenvalue decomposition of the covariance matrix\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "# Whitening transformation\n",
    "whitening_matrix = np.dot(eigenvectors, np.dot(np.diag(1.0/np.sqrt(eigenvalues + 1e-5)), eigenvectors.T))\n",
    "\n",
    "# Whiten the data\n",
    "w_d = np.dot(X_centered, whitening_matrix)\n",
    "Tw_d = np.transpose(w_d)\n",
    "\n",
    "\n",
    "correlation_coefficients = []\n",
    "\n",
    "#for i in range(2):\n",
    "   # Generate some sample data\n",
    "s1 = Tw_d[0]  # Signal 1 : sinusoidal signal\n",
    "s2 = Tw_d[1]  # Signal 2 : square signal\n",
    "s3 = Tw_d[2] \n",
    "s4 = Tw_d[3]\n",
    "s5 = Tw_d[4]\n",
    "s6 = Tw_d[5] \n",
    "s7 = Tw_d[6] \n",
    "s8 = Tw_d[7] \n",
    "s9 = Tw_d[8] \n",
    "#ica = FastICA(n_components=8)\n",
    "#A = ica.fit_transform(Tw_d)\n",
    "\n",
    "   # Mix the signals into a single observed signal\n",
    "   #n=1030\n",
    "S = np.c_[s1, s2, s3 , s4 , s5, s6, s7, s8]\n",
    "#X1 = np.dot(S, A.T)  # Observed signal\n",
    "   \n",
    "\n",
    "\n",
    "##################\n",
    "# Center the data\n",
    "#X_centered = X - np.mean(X, axis=0)\n",
    "\n",
    "# Instantiate and fit the CustomFastICA model\n",
    "custom_ica = CustomFastICA(n_components=8, df=8.869463345651127, loc=-0.0364849978662904, scale=0.8750974313065253)\n",
    "S_ = custom_ica.fit_transform(S)\n",
    " # Fit the model \n",
    "y = df['Concrete compressive strength']\n",
    "X = np.array([S_[:, 0],S_[:, 1],S_[:, 2], S_[:, 3],S_[:, 4],S_[:, 5],S_[:, 6],S_[:, 7]])\n",
    "XT= np.transpose(X)\n",
    "XT = sm.add_constant(XT)  \n",
    "# S_ now contains the independent components computed using CustomFastICA with the t-distribution\n",
    "\n",
    "###################################################\n",
    "# Fit the model using Ridge regression\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Fit the model using Ridge regression\n",
    "alpha = 1.0  # You can adjust the regularization strength as needed\n",
    "ridge_model = Ridge(alpha=alpha)\n",
    "ridge_model.fit(XT, y)\n",
    "\n",
    "# Calculate predicted values\n",
    "y_pred = ridge_model.predict(XT)\n",
    "\n",
    "# Calculate R-squared manually\n",
    "ss_residual = np.sum((y - y_pred) ** 2)\n",
    "ss_total = np.sum((y - np.mean(y)) ** 2)\n",
    "r_squared = 1 - (ss_residual / ss_total)\n",
    "print(\"R-squared:\", r_squared)\n",
    "\n",
    "# Print intercept and coefficients\n",
    "print(\"Intercept:\", ridge_model.intercept_)\n",
    "print(\"Coefficients:\", ridge_model.coef_)\n",
    "\n",
    "########################################################\n",
    "\n",
    "TS_ = np.transpose(S_)\n",
    "#pvalues = ridge_model.pvalues\n",
    "#print(pvalues)\n",
    "plt.subplot(3, 3,1)\n",
    "plt.plot(TS_[0])\n",
    "plt.subplot(3, 3,2)\n",
    "plt.plot(TS_[1])\n",
    "plt.subplot(3, 3,3)\n",
    "plt.plot(TS_[2])\n",
    "plt.subplot(3, 3,4)\n",
    "plt.plot(TS_[3])\n",
    "plt.subplot(3, 3,5)\n",
    "plt.plot(TS_[4])\n",
    "plt.subplot(3, 3,6)\n",
    "plt.plot(TS_[5])\n",
    "plt.subplot(3, 3,7)\n",
    "plt.plot(TS_[6])\n",
    "plt.subplot(3, 3,8)\n",
    "plt.plot(TS_[7])\n",
    "plt.show()\n",
    "\n",
    "   # Print summary \n",
    "\n",
    "# Calculate the residuals\n",
    "residuals = y - ridge_model.predict(XT)\n",
    "\n",
    "# Calculate the standard deviation of the residuals\n",
    "noise_std = np.std(residuals)\n",
    "\n",
    "print(\"Noise level (standard deviation of residuals):\", noise_std)\n",
    "TS_ = np.transpose(S_)\n",
    "#pvalues = ridge_model.pvalues\n",
    "plt.subplot(3, 3,1)\n",
    "plt.hist(TS_[0])\n",
    "plt.subplot(3, 3,2)\n",
    "plt.hist(TS_[1])\n",
    "plt.subplot(3, 3,3)\n",
    "plt.hist(TS_[2])\n",
    "plt.subplot(3, 3,4)\n",
    "plt.hist(TS_[3])\n",
    "plt.subplot(3, 3,5)\n",
    "plt.hist(TS_[4])\n",
    "plt.subplot(3, 3,6)\n",
    "plt.hist(TS_[5])\n",
    "plt.subplot(3, 3,7)\n",
    "plt.hist(TS_[6])\n",
    "plt.subplot(3, 3,8)\n",
    "plt.hist(TS_[7])\n",
    "plt.show()\n",
    "\n",
    "   # calculate correlation coefficient\n",
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from scipy.stats import pearsonr\n",
    "   # calculate Pearson's correlation\n",
    "corr1 = pearsonr(TS_[0], s1)\n",
    "   \n",
    "corr2 = pearsonr(TS_[1], s2)\n",
    "corr3 = pearsonr(TS_[2], s3)\n",
    "corr4 = pearsonr(TS_[3], s4)\n",
    "corr5 = pearsonr(TS_[4], s5)\n",
    "corr6 = pearsonr(TS_[5], s6)\n",
    "corr7 = pearsonr(TS_[6], s7)\n",
    "corr8 = pearsonr(TS_[7], s8)\n",
    "   \n",
    "  \n",
    "\n",
    "correlation_coefficients.append([corr1, corr2, corr3, corr4,corr5,corr6,corr7,corr8])  \n",
    "   \n",
    "\n",
    "  # Assuming vector is your list of numbers\n",
    "  #X = np.array([TS_[0], TS_[1] ,TS_[2], TS_[3],TS_[4],TS_[5],TS_[6],TS_[7],TS_[8]])\n",
    "#XT= np.transpose(X)\n",
    "kurtosis_values = scipy.stats.kurtosis(S_)\n",
    "\n",
    "print(correlation_coefficients)\n",
    "print(\"Kurtosis of the vector:\", kurtosis_values)\n",
    "#print(w_d[0])\n",
    "\n",
    "# Create scatter plot\n",
    "plt.subplot(3, 3,1)\n",
    "plt.scatter(S_[:, 0], S_[:, 1])\n",
    "plt.subplot(3, 3,2)\n",
    "plt.scatter(S_[:, 1], S_[:, 2])\n",
    "plt.subplot(3, 3,3)\n",
    "plt.scatter(S_[:, 2], S_[:, 3])\n",
    "plt.subplot(3, 3,4)\n",
    "plt.scatter(S_[:, 3], S_[:, 4])\n",
    "plt.subplot(3, 3,5)\n",
    "plt.scatter(S_[:, 4], S_[:, 5])\n",
    "plt.subplot(3, 3,6)\n",
    "plt.scatter(S_[:, 5], S_[:, 6])\n",
    "plt.subplot(3, 3,7)\n",
    "plt.scatter(S_[:, 6], S_[:, 7])\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(3, 3,1)\n",
    "plt.scatter(S_[:, 0], y)\n",
    "plt.subplot(3, 3,2)\n",
    "plt.scatter(S_[:, 1], y)\n",
    "plt.subplot(3, 3,3)\n",
    "plt.scatter(S_[:, 2], y)\n",
    "plt.subplot(3, 3,4)\n",
    "plt.scatter(S_[:, 3], y)\n",
    "plt.subplot(3, 3,5)\n",
    "plt.scatter(S_[:, 4], y)\n",
    "plt.subplot(3, 3,6)\n",
    "plt.scatter(S_[:, 5], y)\n",
    "plt.subplot(3, 3,7)\n",
    "plt.scatter(S_[:, 6], y)\n",
    "plt.subplot(3, 3,8)\n",
    "plt.scatter(S_[:, 7], y)\n",
    "plt.show()\n",
    "############################################################\n",
    "import numpy as np\n",
    "\n",
    "def count_outliers(data):\n",
    "    \"\"\"\n",
    "    Count the number of outliers in a dataset using the interquartile range (IQR) method.\n",
    "\n",
    "    Parameters:\n",
    "        data (array-like): Input data (list or numpy array).\n",
    "\n",
    "    Returns:\n",
    "        num_outliers (int): Number of outliers.\n",
    "    \"\"\"\n",
    "    # Convert to numpy array if input is list\n",
    "    data = np.array(data)\n",
    "    \n",
    "    # Calculate the first and third quartiles\n",
    "    q1 = np.percentile(data, 25)\n",
    "    q3 = np.percentile(data, 75)\n",
    "\n",
    "    # Calculate the interquartile range (IQR)\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    # Define the lower and upper bounds for outliers\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "    # Count the number of outliers\n",
    "    num_outliers = np.sum((data < lower_bound) | (data > upper_bound))\n",
    "    return num_outliers\n",
    "\n",
    "# Example usage:\n",
    "data = S_\n",
    "num_outliers = count_outliers(data)\n",
    "print(\"Number of outliers:\", num_outliers)\n",
    "\n",
    "\n",
    "###################################################\n",
    "\n",
    "# Show the plot\n",
    "#plt.xlabel('Third Principal Component')\n",
    "#plt.ylabel('Sixth Principal Component')\n",
    "#plt.title('Scatter Plot of Third vs Sixth Principal Components')\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "############ correlation between y and S_  for noise:\n",
    "corr11 = pearsonr(TS_[0], y)\n",
    "   \n",
    "corr12 = pearsonr(TS_[1], y)\n",
    "corr13 = pearsonr(TS_[2], y)\n",
    "corr14 = pearsonr(TS_[3], y)\n",
    "corr15 = pearsonr(TS_[4], y)\n",
    "corr16 = pearsonr(TS_[5], y)\n",
    "corr17 = pearsonr(TS_[6], y)\n",
    "corr18 = pearsonr(TS_[7], y)\n",
    "   \n",
    "  \n",
    "\n",
    "correlation_coefficients.append([corr11, corr12, corr13, corr14,corr15,corr16,corr17,corr18])  \n",
    "   \n",
    "\n",
    "  # Assuming vector is your list of numbers\n",
    "  #X = np.array([TS_[0], TS_[1] ,TS_[2], TS_[3],TS_[4],TS_[5],TS_[6],TS_[7],TS_[8]])\n",
    "#XT= np.transpose(X)\n",
    "\n",
    "print(correlation_coefficients)\n",
    "\n",
    "skewness_values = scipy.stats.skew(S_, axis=0)\n",
    "print(\"Skewness of the independent components:\")\n",
    "print(skewness_values)\n",
    "\n",
    "\n",
    "########################3\n",
    "#qqplot for cosider symmetric of S_\n",
    "\n",
    "# Calculate the residuals\n",
    "residuals = y -  ridge_model.predict(XT)\n",
    "\n",
    "# Generate Q-Q plots for the residuals\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for i in range(8):\n",
    "    ax = axes[i // 4, i % 4]\n",
    "    qqplot(residuals, line='s', ax=ax)\n",
    "    ax.set_title(f\"Residuals Q-Q Plot for Component {i+1}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "##########################################\n",
    "#Anova for realize nonlinear relationship between y and S_:\n",
    "\n",
    "# Generate polynomial features for the independent variables\n",
    "poly = PolynomialFeatures(degree=2)  # You can adjust the degree as needed\n",
    "S_poly = poly.fit_transform(S_)\n",
    "\n",
    "# Fit the ANOVA model with polynomial terms\n",
    "model_anova = sm.OLS(y, S_poly).fit()\n",
    "\n",
    "# Print summary of ANOVA model\n",
    "print(model_anova.summary())\n",
    "\n",
    "\n",
    "##################################################\n",
    "# plot of residual against of predicted value:\n",
    "#####################################\n",
    "# Assuming S_ contains the independent components computed using CustomFastICA with the t-distribution\n",
    "\n",
    "# Reshape the first column of S_ to a 2D array with a single feature\n",
    "X_first_column = S_[:, 0].reshape(-1, 1)\n",
    "X_two_column = S_[:, 1].reshape(-1, 1)\n",
    "X_three_column = S_[:, 2].reshape(-1, 1)\n",
    "X_four_column = S_[:, 3].reshape(-1, 1)\n",
    "X_five_column = S_[:, 4].reshape(-1, 1)\n",
    "X_six_column = S_[:, 5].reshape(-1, 1)\n",
    "X_seven_column = S_[:, 6].reshape(-1, 1)\n",
    "X_eight_column = S_[:, 7].reshape(-1, 1)\n",
    "\n",
    "# Fit a linear regression model\n",
    "#model = LinearRegression()\n",
    "ridge_model.fit(X_first_column, y)\n",
    "ridge_model.fit(X_two_column, y)\n",
    "ridge_model.fit(X_three_column, y)\n",
    "ridge_model.fit(X_four_column, y)\n",
    "ridge_model.fit(X_five_column , y)\n",
    "ridge_model.fit(X_six_column, y)\n",
    "ridge_model.fit(X_seven_column, y)\n",
    "ridge_model.fit(X_eight_column, y)\n",
    "\n",
    "# Calculate residuals\n",
    "residuals1 = y - ridge_model.predict(X_first_column)\n",
    "residuals2 = y - ridge_model.predict(X_two_column)\n",
    "residuals3 = y - ridge_model.predict(X_three_column)\n",
    "residuals4 = y - ridge_model.predict(X_four_column)\n",
    "residuals5 = y - ridge_model.predict(X_five_column )\n",
    "residuals6 = y - ridge_model.predict(X_six_column)\n",
    "residuals7 = y - ridge_model.predict(X_seven_column)\n",
    "residuals8= y - ridge_model.predict(X_eight_column)\n",
    "\n",
    "# Display the model coefficients\n",
    "print(\"Model Coefficients:\", ridge_model.coef_)\n",
    "\n",
    "# Display the residuals\n",
    "#print(\"Residuals:\")\n",
    "#for value in residuals:\n",
    "#    print(value)\n",
    "    \n",
    "# Plot residuals vs. predicted values\n",
    "predicted_values1 = ridge_model.predict(X_first_column)\n",
    "predicted_values2 = ridge_model.predict(X_two_column)\n",
    "predicted_values3 = ridge_model.predict(X_three_column)\n",
    "predicted_values4 = ridge_model.predict(X_four_column)\n",
    "predicted_values5 = ridge_model.predict(X_five_column)\n",
    "predicted_values6 = ridge_model.predict(X_six_column)\n",
    "predicted_values7 = ridge_model.predict(X_seven_column)\n",
    "predicted_values8 = ridge_model.predict(X_eight_column)\n",
    "\n",
    "plt.subplot(3, 3,1)\n",
    "plt.scatter(predicted_values1, residuals1)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "#plt.title(\"Residual Plot\")\n",
    "plt.axhline(y=0, color='r', linestyle='-')  # Add horizontal line at y=0\n",
    "\n",
    "plt.subplot(3, 3,2)\n",
    "plt.scatter(predicted_values2, residuals2)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "#plt.title(\"Residual Plot\")\n",
    "plt.axhline(y=0, color='r', linestyle='-')  # Add horizontal line at y=0\n",
    "\n",
    "plt.subplot(3, 3,3)\n",
    "plt.scatter(predicted_values3, residuals3)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "#plt.title(\"Residual Plot\")\n",
    "plt.axhline(y=0, color='r', linestyle='-')  # Add horizontal line at y=0\n",
    "\n",
    "plt.subplot(3, 3,4)\n",
    "plt.scatter(predicted_values4, residuals4)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "#plt.title(\"Residual Plot\")\n",
    "plt.axhline(y=0, color='r', linestyle='-')  # Add horizontal line at y=0\n",
    "\n",
    "plt.subplot(3, 3,5)\n",
    "plt.scatter(predicted_values5, residuals5)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "#plt.title(\"Residual Plot\")\n",
    "plt.axhline(y=0, color='r', linestyle='-')  # Add horizontal line at y=0\n",
    "\n",
    "plt.subplot(3, 3,6)\n",
    "plt.scatter(predicted_values6, residuals6)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "#plt.title(\"Residual Plot\")\n",
    "plt.axhline(y=0, color='r', linestyle='-')  # Add horizontal line at y=0\n",
    "\n",
    "plt.subplot(3, 3,7)\n",
    "plt.scatter(predicted_values7, residuals7)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "#plt.title(\"Residual Plot\")\n",
    "plt.axhline(y=0, color='r', linestyle='-')  # Add horizontal line at y=0\n",
    "\n",
    "plt.subplot(3, 3,8)\n",
    "plt.scatter(predicted_values8, residuals8)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "#plt.title(\"Residual Plot\")\n",
    "plt.axhline(y=0, color='r', linestyle='-')  # Add horizontal line at y=0\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd17d4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## concrete data\n",
    "# polynomial regression in icr:\n",
    "###############################################################################################\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#####################################################\n",
    "####################################################\n",
    "\n",
    "\n",
    "# Fit the polynomial regression model\n",
    "# Fit the model using statsmodels\n",
    "y = df['Concrete compressive strength']\n",
    "X = np.array([S_[:, 0],S_[:, 1],S_[:, 2], S_[:, 3],S_[:, 4],S_[:, 5],S_[:, 6],S_[:, 7]])\n",
    "XT= np.transpose(X)\n",
    "XT = sm.add_constant(XT) \n",
    "\n",
    "\n",
    "\n",
    "# Fit the polynomial regression model using statsmodels\n",
    "poly_degree = 3  # Adjust the polynomial degree as needed\n",
    "poly = PolynomialFeatures(degree=poly_degree)\n",
    "XT_poly = poly.fit_transform(XT)\n",
    "\n",
    "poly_regression_model_sm = sm.OLS(y, XT_poly).fit()\n",
    "\n",
    "# Fit the model using polynomial regression from scikit-learn\n",
    "poly_regression_model = LinearRegression()\n",
    "poly_regression_model.fit(XT_poly, y)\n",
    "\n",
    "# Calculate predicted values\n",
    "y_pred_poly = poly_regression_model.predict(XT_poly)\n",
    "\n",
    "# Calculate R-squared for polynomial regression\n",
    "ss_residual_poly = np.sum((y - y_pred_poly) ** 2)\n",
    "ss_total = np.sum((y - np.mean(y)) ** 2)\n",
    "r_squared_poly = 1 - (ss_residual_poly / ss_total)\n",
    "print(\"R-squared (Polynomial Regression):\", r_squared_poly)\n",
    "\n",
    "\n",
    "# Print intercept and coefficients for polynomial regression\n",
    "print(\"Intercept (Polynomial Regression):\", poly_regression_model.intercept_)\n",
    "print(\"Coefficients (Polynomial Regression):\", poly_regression_model.coef_)\n",
    "\n",
    "# Plot actual vs. predicted values\n",
    "plt.scatter(y, y_pred_poly)\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs. Predicted Values (Polynomial Regression)\")\n",
    "plt.show()\n",
    "\n",
    "# Plot residuals vs. predicted values\n",
    "residuals_poly = y - y_pred_poly\n",
    "plt.scatter(y_pred_poly, residuals_poly)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot (Polynomial Regression)\")\n",
    "plt.axhline(y=0, color='r', linestyle='-')  # Add horizontal line at y=0\n",
    "plt.show()\n",
    "\n",
    "# Generate Q-Q plots for the residuals\n",
    "fig, ax = plt.subplots()\n",
    "qqplot(residuals_poly, line='s', ax=ax)\n",
    "ax.set_title(\"Residuals Q-Q Plot (Polynomial Regression)\")\n",
    "plt.show()\n",
    "\n",
    "# Print summary of polynomial regression model\n",
    "print(poly_regression_model_sm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd395621",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Table 5\n",
    "###########################################\n",
    "## ICA on heart data after transformation, normalization, and noise removal\n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('h1.csv')\n",
    "dff = pd.read_csv('h11.csv')\n",
    "\n",
    "# 1. Data distribution correction with PowerTransformer (Yeo-Johnson)\n",
    "pt = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "df_transformed = pt.fit_transform(df)\n",
    "\n",
    "# 2. Normalization (Standardization)\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df_transformed)\n",
    "\n",
    "# 3. Detect noisy samples (outliers) based on IQR on each sample (row)\n",
    "def detect_noisy_samples(data, threshold=0.3):\n",
    "    noisy_indices = []\n",
    "    for i in range(data.shape[0]):\n",
    "        sample = data[i, :]\n",
    "        q1 = np.percentile(sample, 25)\n",
    "        q3 = np.percentile(sample, 75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        outlier_flags = (sample < lower_bound) | (sample > upper_bound)\n",
    "        outlier_ratio = np.sum(outlier_flags) / data.shape[1]\n",
    "        if outlier_ratio > threshold:\n",
    "            noisy_indices.append(i)\n",
    "    return noisy_indices\n",
    "\n",
    "noisy_samples = detect_noisy_samples(df_scaled, threshold=0.3)\n",
    "print(f\"Number of detected noisy samples: {len(noisy_samples)}\")\n",
    "\n",
    "# 4. Remove noisy samples from data and target variable\n",
    "df_scaled_clean = np.delete(df_scaled, noisy_samples, axis=0)\n",
    "y_clean = np.delete(dff['DEATH_EVENT'].values, noisy_samples, axis=0)\n",
    "\n",
    "# 5. Run ICA on cleaned data\n",
    "ica = FastICA(n_components=11, max_iter=5000, tol=2e-3, random_state=42)\n",
    "S_ = ica.fit_transform(df_scaled_clean)\n",
    "A = ica.mixing_\n",
    "\n",
    "# 6. Display independent components (ICs)\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i in range(11):\n",
    "    plt.subplot(4, 3, i+1)\n",
    "    plt.plot(S_[:, i])\n",
    "    plt.title(f'IC {i+1}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Regression model using ICs for the cleaned target variable\n",
    "X = sm.add_constant(S_)\n",
    "model = sm.OLS(y_clean, X).fit()\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# 8. Calculate VIF to check multicollinearity\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = [f\"IC{i+1}\" for i in range(S_.shape[1])]\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(S_, i) for i in range(S_.shape[1])]\n",
    "print(vif_data)\n",
    "\n",
    "# 9. Check correlation between each IC and its corresponding original signal\n",
    "correlation_coefficients = []\n",
    "for i in range(11):\n",
    "    corr = pearsonr(S_[:, i], df_scaled_clean[:, i])\n",
    "    correlation_coefficients.append(corr)\n",
    "print(\"Correlation coefficients (IC vs original feature):\")\n",
    "for i, corr in enumerate(correlation_coefficients):\n",
    "    print(f\"IC{i+1} vs Feature{i+1}: r={corr[0]:.3f}, p={corr[1]:.3e}\")\n",
    "\n",
    "# 10. Check number of outliers in independent components\n",
    "def count_outliers(data):\n",
    "    q1 = np.percentile(data, 25)\n",
    "    q3 = np.percentile(data, 75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    return np.sum((data < lower_bound) | (data > upper_bound))\n",
    "\n",
    "outliers_per_ic = [count_outliers(S_[:, i]) for i in range(S_.shape[1])]\n",
    "print(\"Number of outliers per IC:\", outliers_per_ic)\n",
    "\n",
    "# 11. Histogram of independent components\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i in range(11):\n",
    "    plt.subplot(4, 3, i+1)\n",
    "    plt.hist(S_[:, i], bins=30)\n",
    "    plt.title(f'Histogram IC {i+1}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8537c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Table 5 \n",
    "\n",
    "##############################################################\n",
    "# ICA using Ridge, Lasso, and Polynomial Regression\n",
    "########################################################################\n",
    "\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('h1.csv')\n",
    "dff = pd.read_csv('h11.csv')\n",
    "\n",
    "# 1. Data distribution correction with PowerTransformer (Yeo-Johnson)\n",
    "pt = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "df_transformed = pt.fit_transform(df)\n",
    "\n",
    "# 2. Normalization (Standardization)\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df_transformed)\n",
    "\n",
    "# 3. Detect noisy samples (outliers) based on IQR on each sample (row)\n",
    "def detect_noisy_samples(data, threshold=0.3):\n",
    "    noisy_indices = []\n",
    "    for i in range(data.shape[0]):\n",
    "        sample = data[i, :]\n",
    "        q1 = np.percentile(sample, 25)\n",
    "        q3 = np.percentile(sample, 75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        outlier_flags = (sample < lower_bound) | (sample > upper_bound)\n",
    "        outlier_ratio = np.sum(outlier_flags) / data.shape[1]\n",
    "        if outlier_ratio > threshold:\n",
    "            noisy_indices.append(i)\n",
    "    return noisy_indices\n",
    "\n",
    "noisy_samples = detect_noisy_samples(df_scaled, threshold=0.3)\n",
    "print(f\"Number of detected noisy samples: {len(noisy_samples)}\")\n",
    "\n",
    "# 4. Remove noisy samples from data and target variable\n",
    "df_scaled_clean = np.delete(df_scaled, noisy_samples, axis=0)\n",
    "y_clean = np.delete(dff['DEATH_EVENT'].values, noisy_samples, axis=0)\n",
    "\n",
    "# 5. Run ICA on cleaned data\n",
    "ica = FastICA(n_components=11, max_iter=5000, tol=2e-3, random_state=42)\n",
    "S_ = ica.fit_transform(df_scaled_clean)\n",
    "A = ica.mixing_\n",
    "\n",
    "# 6. Display independent components (ICs)\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i in range(11):\n",
    "    plt.subplot(4, 3, i+1)\n",
    "    plt.plot(S_[:, i])\n",
    "    plt.title(f'IC {i+1}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Define a function to report regression results\n",
    "def regression_report(name, model, X, y_true):\n",
    "    y_pred = model.predict(X)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    print(f\"--- Results for {name} ---\")\n",
    "    print(f\"R2 score: {r2:.4f}\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(\"Coefficients:\")\n",
    "    if hasattr(model, 'coef_'):\n",
    "        print(model.coef_)\n",
    "    if hasattr(model, 'intercept_'):\n",
    "        print(f\"Intercept: {model.intercept_}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# 7. Ridge Regression\n",
    "ridge = Ridge(alpha=1.0, random_state=42)\n",
    "ridge.fit(S_, y_clean)\n",
    "regression_report(\"Ridge Regression\", ridge, S_, y_clean)\n",
    "\n",
    "# 8. Lasso Regression\n",
    "lasso = Lasso(alpha=0.1, random_state=42, max_iter=10000)\n",
    "lasso.fit(S_, y_clean)\n",
    "regression_report(\"Lasso Regression\", lasso, S_, y_clean)\n",
    "\n",
    "# 9. Polynomial Regression (degree 2)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "S_poly = poly.fit_transform(S_)\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(S_poly, y_clean)\n",
    "regression_report(\"Polynomial Regression (degree=2)\", lin_reg, S_poly, y_clean)\n",
    "\n",
    "# 10. Calculate VIF to check multicollinearity (for original ICs)\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = [f\"IC{i+1}\" for i in range(S_.shape[1])]\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(S_, i) for i in range(S_.shape[1])]\n",
    "print(\"VIF values for IC features:\")\n",
    "print(vif_data)\n",
    "\n",
    "# 11. Check correlation between each IC and its corresponding original signal\n",
    "correlation_coefficients = []\n",
    "for i in range(11):\n",
    "    corr = pearsonr(S_[:, i], df_scaled_clean[:, i])\n",
    "    correlation_coefficients.append(corr)\n",
    "print(\"Correlation coefficients (IC vs original feature):\")\n",
    "for i, corr in enumerate(correlation_coefficients):\n",
    "    print(f\"IC{i+1} vs Feature{i+1}: r={corr[0]:.3f}, p={corr[1]:.3e}\")\n",
    "\n",
    "# 12. Check number of outliers in independent components\n",
    "def count_outliers(data):\n",
    "    q1 = np.percentile(data, 25)\n",
    "    q3 = np.percentile(data, 75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    return np.sum((data < lower_bound) | (data > upper_bound))\n",
    "\n",
    "outliers_per_ic = [count_outliers(S_[:, i]) for i in range(S_.shape[1])]\n",
    "print(\"Number of outliers per IC:\", outliers_per_ic)\n",
    "\n",
    "# 13. Histogram of independent components\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i in range(11):\n",
    "    plt.subplot(4, 3, i+1)\n",
    "    plt.hist(S_[:, i], bins=30)\n",
    "    plt.title(f'Histogram IC {i+1}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94b4119",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Table 5\n",
    "########################################################################\n",
    "\n",
    "#Spline Regression\n",
    "#####################################\n",
    "\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, PolynomialFeatures, SplineTransformer\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('h1.csv')\n",
    "dff = pd.read_csv('h11.csv')\n",
    "\n",
    "# 1. Data distribution correction with PowerTransformer (Yeo-Johnson)\n",
    "pt = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "df_transformed = pt.fit_transform(df)\n",
    "\n",
    "# 2. Normalization (Standardization)\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df_transformed)\n",
    "\n",
    "# 3. Detect noisy samples (outliers) based on IQR on each sample (row)\n",
    "def detect_noisy_samples(data, threshold=0.3):\n",
    "    noisy_indices = []\n",
    "    for i in range(data.shape[0]):\n",
    "        sample = data[i, :]\n",
    "        q1 = np.percentile(sample, 25)\n",
    "        q3 = np.percentile(sample, 75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        outlier_flags = (sample < lower_bound) | (sample > upper_bound)\n",
    "        outlier_ratio = np.sum(outlier_flags) / data.shape[1]\n",
    "        if outlier_ratio > threshold:\n",
    "            noisy_indices.append(i)\n",
    "    return noisy_indices\n",
    "\n",
    "noisy_samples = detect_noisy_samples(df_scaled, threshold=0.3)\n",
    "print(f\"Number of detected noisy samples: {len(noisy_samples)}\")\n",
    "\n",
    "# 4. Remove noisy samples from data and target variable\n",
    "df_scaled_clean = np.delete(df_scaled, noisy_samples, axis=0)\n",
    "y_clean = np.delete(dff['DEATH_EVENT'].values, noisy_samples, axis=0)\n",
    "\n",
    "# 5. Run ICA on cleaned data\n",
    "ica = FastICA(n_components=11, max_iter=5000, tol=2e-3, random_state=42)\n",
    "S_ = ica.fit_transform(df_scaled_clean)\n",
    "A = ica.mixing_\n",
    "\n",
    "# 6. Display independent components (ICs)\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i in range(11):\n",
    "    plt.subplot(4, 3, i+1)\n",
    "    plt.plot(S_[:, i])\n",
    "    plt.title(f'IC {i+1}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Define function to report regression results\n",
    "def regression_report(name, model, X, y_true):\n",
    "    y_pred = model.predict(X)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    print(f\"--- Results for {name} ---\")\n",
    "    print(f\"R2 score: {r2:.4f}\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(\"Coefficients:\")\n",
    "    if hasattr(model, 'coef_'):\n",
    "        print(model.coef_)\n",
    "    if hasattr(model, 'intercept_'):\n",
    "        print(f\"Intercept: {model.intercept_}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# 7. Ridge Regression\n",
    "ridge = Ridge(alpha=1.0, random_state=42)\n",
    "ridge.fit(S_, y_clean)\n",
    "regression_report(\"Ridge Regression\", ridge, S_, y_clean)\n",
    "\n",
    "# 8. Lasso Regression\n",
    "lasso = Lasso(alpha=0.1, random_state=42, max_iter=10000)\n",
    "lasso.fit(S_, y_clean)\n",
    "regression_report(\"Lasso Regression\", lasso, S_, y_clean)\n",
    "\n",
    "# 9. Polynomial Regression (degree 2)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "S_poly = poly.fit_transform(S_)\n",
    "\n",
    "lin_reg_poly = LinearRegression()\n",
    "lin_reg_poly.fit(S_poly, y_clean)\n",
    "regression_report(\"Polynomial Regression (degree=2)\", lin_reg_poly, S_poly, y_clean)\n",
    "\n",
    "# 10. Spline Regression (with 4 internal knots)\n",
    "spline = SplineTransformer(degree=3, n_knots=7, include_bias=False)\n",
    "S_spline = spline.fit_transform(S_)\n",
    "\n",
    "lin_reg_spline = LinearRegression()\n",
    "lin_reg_spline.fit(S_spline, y_clean)\n",
    "regression_report(\"Spline Regression (degree=3)\", lin_reg_spline, S_spline, y_clean)\n",
    "\n",
    "# 11. Calculate VIF to check multicollinearity (for original ICs)\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = [f\"IC{i+1}\" for i in range(S_.shape[1])]\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(S_, i) for i in range(S_.shape[1])]\n",
    "print(\"VIF values for IC features:\")\n",
    "print(vif_data)\n",
    "\n",
    "# 12. Check correlation between each IC and its corresponding original signal\n",
    "correlation_coefficients = []\n",
    "for i in range(11):\n",
    "    corr = pearsonr(S_[:, i], df_scaled_clean[:, i])\n",
    "    correlation_coefficients.append(corr)\n",
    "print(\"Correlation coefficients (IC vs original feature):\")\n",
    "for i, corr in enumerate(correlation_coefficients):\n",
    "    print(f\"IC{i+1} vs Feature{i+1}: r={corr[0]:.3f}, p={corr[1]:.3e}\")\n",
    "\n",
    "# 13. Check number of outliers in independent components\n",
    "def count_outliers(data):\n",
    "    q1 = np.percentile(data, 25)\n",
    "    q3 = np.percentile(data, 75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    return np.sum((data < lower_bound) | (data > upper_bound))\n",
    "\n",
    "outliers_per_ic = [count_outliers(S_[:, i]) for i in range(S_.shape[1])]\n",
    "print(\"Number of outliers per IC:\", outliers_per_ic)\n",
    "\n",
    "# 14. Histogram of independent components\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i in range(11):\n",
    "    plt.subplot(4, 3, i+1)\n",
    "    plt.hist(S_[:, i], bins=30)\n",
    "    plt.title(f'Histogram IC {i+1}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbd8dca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
